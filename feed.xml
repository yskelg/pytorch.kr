<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://pytorch.kr/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pytorch.kr/" rel="alternate" type="text/html" hreflang="ko" /><updated>2024-08-29T03:10:46+09:00</updated><id>https://pytorch.kr/feed.xml</id><title type="html">파이토치 한국 사용자 모임 (PyTorch Korea User Group)</title><subtitle>파이토치 한국 사용자 모임에 오신 것을 환영합니다. 딥러닝 프레임워크인 파이토치(PyTorch)를 사용하는 한국어 사용자들을 위해 문서를 번역하고 정보를 공유하고 있습니다.</subtitle><author><name>PyTorch Korea User Group</name></author><entry><title type="html">토치챗(torchchat) 소개: 노트북, 데스크탑 및 모바일에서 로컬 LLM 추론 가속화하기</title><link href="https://pytorch.kr/blog/2024/torchchat-local-llm-inference/" rel="alternate" type="text/html" title="토치챗(torchchat) 소개: 노트북, 데스크탑 및 모바일에서 로컬 LLM 추론 가속화하기" /><published>2024-07-30T00:00:00+09:00</published><updated>2024-07-30T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2024/torchchat-local-llm-inference</id><content type="html" xml:base="https://pytorch.kr/blog/2024/torchchat-local-llm-inference/">&lt;p&gt;오늘 노트북과 데스크탑, 모바일에서 Llama 3와 3.1, 그리고 다른 대규모 언어 모델(LLM, Large Language Model)을 원활하고 고성능으로 실행하는 방법을 보여주는 라이브러리인 &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;torchchat&lt;/a&gt;을 출시했습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Today, we’re releasing &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;torchchat&lt;/a&gt;, a library showcasing how to seamlessly and performantly run Llama 3, 3.1, and other large language models across laptop, desktop, and mobile.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이전 블로그 게시물에서는 네이티브 PyTorch 2에서 CUDA를 활용하여 LLM을 뛰어난 성능으로 실행하는 것을 &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;보여드렸었습니다&lt;/a&gt;. Torchchat은 이를 더 많은 대상 환경과 모델, 실행 모드에서 확장했습니다. 또한, 내보내기(export)나 양자화(quantization), 평가(eval)와 같은 주요 기능들을 이해하기 쉬운 방식으로 제공하여 로컬 추론 솔루션을 구축하려는 사람들에게 시작부터 끝까지 알려(E2E story)드립니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In our previous blog posts, we &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;showed&lt;/a&gt; how to use native PyTorch 2 to run LLMs with great performance using CUDA. Torchchat expands on this with more target environments, models and execution modes. Additionally it provides important functions such as export, quantization and eval in a way that’s easy to understand providing an E2E story for those who want to build a local inference solution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Torchchat은 3가지 영역으로 구성되어 있습니다:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You will find the project organized into three areas:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Python: Torchchat은 Python CLI를 통해 호춣하거나 브라우저로 접근할 수 있는 &lt;a href=&quot;https://github.com/pytorch/torchchat?tab=readme-ov-file#server&quot;&gt;REST API&lt;/a&gt;를 제공합니다.
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Python: Torchchat provides a &lt;a href=&quot;https://github.com/pytorch/torchchat?tab=readme-ov-file#server&quot;&gt;REST API&lt;/a&gt; that is called via a Python CLI or can be accessed via the browser&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;C++: 파이토치(PyTorch)의 &lt;a href=&quot;https://pytorch-dev-podcast.simplecast.com/episodes/aotinductor&quot;&gt;AOTInductor&lt;/a&gt; 백엔드를 사용하여 데스크탑용 바이너리(desktop-friendly binary)를 생성합니다.
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;C++: Torchchat produces a desktop-friendly binary using PyTorch’s &lt;a href=&quot;https://pytorch-dev-podcast.simplecast.com/episodes/aotinductor&quot;&gt;AOTInductor&lt;/a&gt; backend&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;모바일 기기: Torchchat은 &lt;a href=&quot;https://pytorch.org/executorch/stable/index.html&quot;&gt;ExecuTorch&lt;/a&gt; 를 사용하여 온디바이스 추론을 위한 .pte 바이너리 파일을 내보냅니다.
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Mobile devices: Torchchat uses &lt;a href=&quot;https://pytorch.org/executorch/stable/index.html&quot;&gt;ExecuTorch&lt;/a&gt; to export a .pte binary file for on-device inference&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torchchat.png&quot; alt=&quot;토치챗 구조 / torchchat schema&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;성능--performance&quot;&gt;성능 / Performance&lt;/h2&gt;

&lt;p&gt;다음 표는 다양한 구성에서의 Llama 3의 torchchat 성능을 살펴봅니다.
&lt;em&gt;Llama 3.1에서의 수치들은 곧 공개 예정입니다.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The following table tracks the performance of torchchat for Llama 3 for a variety of configurations.&lt;br /&gt;
&lt;em&gt;Numbers for Llama 3.1 are coming soon.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;애플 맥북 M1 Max 64GB 노트북에서 Llama3 8B Instruct 성능 / Llama 3 8B Instruct on Apple MacBook Pro M1 Max 64GB Laptop&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Mode&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;DType&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama 3 8B Tokens/Sec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Arm Compile
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;5.84
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;1.63
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;3.99
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Arm AOTI
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;4.05
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;1.05
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;3.28
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;MPS Eager
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;12.63
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;16.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;17.15
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;CUDA 및 Linux x86에서 Llama3 8B Instruct 성능 / Llama 3 8B Instruct on Linux x86 and CUDA&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz with 180GB Ram + A100 (80GB)&lt;/em&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Mode&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;DType&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama 3 8B Tokens/Sec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;x86 Compile
   &lt;/td&gt;
   &lt;td&gt;bfloat16
   &lt;/td&gt;
   &lt;td&gt;2.76
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;3.15
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;5.33
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;CUDA Compile
   &lt;/td&gt;
   &lt;td&gt;bfloat16
   &lt;/td&gt;
   &lt;td&gt;83.23
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;118.17
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;135.16
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;모바일에서 Llama3 8B Instruct 성능 / Llama3 8B Instruct on Mobile&lt;/strong&gt;&lt;br /&gt;
Torchchat은 ExecuTorch를 통해 4-bit GPTQ를 사용하여 삼성 갤럭시 S23 및 아이폰에서 초당 8T 이상의 속도를 달성했습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Torchchat achieves &amp;gt; 8T/s on the Samsung Galaxy S23 and iPhone using 4-bit GPTQ via ExecuTorch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;결론--conclusion&quot;&gt;결론 / Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;torchchat 저장소를 복제하여 사용해보시기&lt;/a&gt;&lt;/strong&gt;를 권해드립니다. torchchat의 기능을 살펴보고, 파이토치 커뮤니티(PyTorch community)가 로컬 및 제약이 있는 기기(constrained device)에서 LLM을 실행할 수 있도록 역량을 강화하는 과정에 대한 여러분의 피드백을 공유해주시기 바랍니다. 생성형 AI와 LLM의 잠재력을 모든 기기에서 발휘할 수 있도록 함께 해주세요! 빠르게 반복하는 과정 중에 있으므로, 발견하시는 내용들은 &lt;a href=&quot;https://github.com/pytorch/torchat/issues&quot;&gt;이슈&lt;/a&gt;로 남겨주세요. 또한, 모델 추가를 비롯하여 지원하는 하드웨어, 새로운 양자화 기법, 성능 개선 등의 광범위한 범위에 걸친 커뮤니티의 기여를 기다리고 있습니다. 즐거운 실험되시길 바랍니다!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We encourage you to &lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;clone the torchchat repo and give it a spin&lt;/a&gt;&lt;/strong&gt;, explore its capabilities, and share your feedback as we continue to empower the PyTorch community to run LLMs locally and on constrained devices. Together, let’s unlock the full potential of generative AI and LLMs on any device. Please submit &lt;a href=&quot;https://github.com/pytorch/torchat/issues&quot;&gt;issues&lt;/a&gt; as you see them, since we are still iterating quickly. We’re also inviting community contributions across a broad range of areas, from additional models, target hardware support, new quantization schemes, or performance improvements.  Happy experimenting!&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>PyTorch Korea User Group</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">오늘 노트북과 데스크탑, 모바일에서 Llama 3와 3.1, 그리고 다른 대규모 언어 모델(LLM, Large Language Model)을 원활하고 고성능으로 실행하는 방법을 보여주는 라이브러리인 torchchat을 출시했습니다. Today, we’re releasing torchchat, a library showcasing how to seamlessly and performantly run Llama 3, 3.1, and other large language models across laptop, desktop, and mobile.</summary></entry><entry><title type="html">FlashAttention-3: 비동기 및 저정밀도에서의 빠르고 정확한 어텐션 제공</title><link href="https://pytorch.kr/blog/2024/flashattention-3/" rel="alternate" type="text/html" title="FlashAttention-3: 비동기 및 저정밀도에서의 빠르고 정확한 어텐션 제공" /><published>2024-07-11T00:00:00+09:00</published><updated>2024-07-11T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2024/flashattention-3</id><content type="html" xml:base="https://pytorch.kr/blog/2024/flashattention-3/">&lt;p&gt;어텐션(Attention)은 트랜스포머(Transformer) 구조의 핵심 계층(layer)이지만, 대규모 언어 모델(LLM, Large Language Model)과 긴-컨텍스트 애플리케이션(long-context application)의 병목(bottleneck)이기도 합니다. FlashAttention (및 FlashAttention-2)은 메모리 읽기/쓰기를 최소화하여 GPU에서 어텐션 연산을 가속화하는 방법을 개척했으며, 이제 대부분의 &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;라이브러리&lt;/a&gt;에서 트랜스포머(Transformer) 학습 및 추론을 가속화하는데 사용되고 있습니다. 이 덕분에 지난 2년 동안 LLM 컨텍스트 길이가 2-4K(GPT-3, OPT)부터 128K(GPT-4) 및 1M(&lt;a href=&quot;https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k&quot;&gt;Llama 3&lt;/a&gt;)까지 급격히 할 수 있었습니다. 그러나 이러한 성공에도 불구하고, FlashAttention(플래시어텐션)은 최신 하드웨어의 새로운 기능을 아직 활용하지 못했습니다. FlashAttention-2는 H100 GPU에서 이론적 최대 FLOPS(FLoating-point Operations Per Second)의 35%만 활용했습니다. 이 블로그 글에서는 Hopper(호퍼, H100) GPU에서 어텐션 연산을 가속화하기 위한 세 가지 주요 기술을 설명합니다: (1) 워프 특수화(warp-specialization)를 통해 전체 연산과 데이터 이동을 중첩(overlap)하고, (2) 블록-단위 MatMul(Block-wise MatMul) 및 Softmax 연산을 교차로 수행하며, (3) 저정밀도(low-precision) FP8을 위한 하드웨어 지원을 활용하는 비일관적 처리(incoherent processing)입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;libraries&lt;/a&gt; to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (&lt;a href=&quot;https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k&quot;&gt;Llama 3&lt;/a&gt;). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이러한 기법들을 활용한 FlashAttention-3을 발표하게 되어 기쁩니다. FlashAttention-3은 FP16에서 FlashAttention-2보다 1.5-2.0배 빠르며, 최대 740 TFLOPS(TeraFLOPS)를 달성합니다. 즉, H100의 이론적 최대 FLOPS의 75%까지 활용할 수 있습니다. FP8에서 FlashAttention-3은 기존(baseline) FP8 어텐션 연산보다 오차가 2.6배 작으며 1.2 PFLOPS(PetaFLOPS)에 가까운 성능을 달성합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We’re excited to release FlashAttention-3 that incorporates these techniques. It’s 1.5-2.0x faster than FlashAttention-2 with FP16, up to 740 TFLOPS, i.e., 75% utilization of H100 theoretical max FLOPS. With FP8, FlashAttention-3 reaches close to 1.2 PFLOPS, with 2.6x smaller error than baseline FP8 attention.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;FlashAttention-3는 &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;https://github.com/Dao-AILab/flash-attention&lt;/a&gt;에서 확인하실 수 있으며, 논문은 &lt;a href=&quot;https://tridao.me/publications/flash3/flash3.pdf&quot;&gt;여기&lt;/a&gt;에서 확인하실 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;FlashAttention-3 is available at: &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;https://github.com/Dao-AILab/flash-attention&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://tridao.me/publications/flash3/flash3.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;flashattention-돌아보기--flashattention-recap&quot;&gt;FlashAttention 돌아보기 / FlashAttention Recap&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention/플래시어텐션&lt;/a&gt;은 어텐션 연산을 재배치(reorder)하고 타일링(tiling) 및 재계산(recomputation)을 활용하여 시퀀스 길이에 따른 메모리 사용량을 제곱(quadratic)에서 선형(linear)으로 줄여 속도를 크게 향상시키는 알고리즘입니다. 우리는 타일링을 사용하여 HBM(GPU 메모리)에서 SRAM(빠른 캐시)으로 입력 블록을 불러오고 해당 블록에 대한 어텐션 연산을 수행하며 출력을 HBM에 갱신합니다. 중간 단계의 어텐션 행렬을 HBM에 쓰지 않음으로써 메모리 읽기/쓰기 양을 줄여 연산 시간(wallclock time)을 2-4배 빠르게 하였습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; is an algorithm that reorders the attention computation and leverages tiling and recomputation to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. We use tiling to load blocks of inputs from HBM (GPU memory) to SRAM (fast cache), perform attention with respect to that block, and update the output in HBM. By not writing the large intermediate attention matrices to HBM, we reduce the amount of memory reads/writes, which brings 2-4x wallclock time speedup.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;다음은 FlashAttention의 순전파(forward) 연산을 보여주는 다이어그램입니다: 타일링(tiling) 및 softmax 재설계(rescaling)를 함으로써 블록별로 연산을 처리하고 HBM에 읽기/쓰기를 피함으로써 근사치가 아닌 정확한 출력을 얻을 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Here we show a diagram of FlashAttention forward pass: with tiling and softmax rescaling, we operate by blocks and avoid having to read/write from HBM, while obtaining the correct output with no approximation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg1.png&quot; alt=&quot;FlashAttention의 순전파(forward) 연산 수식을 표현한 그림 / math equations&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;hopper-gpu에서의-새로운-하드웨어-기능---wgmma-tma-fp8--new-hardware-features-on-hopper-gpus---wgmma-tma-fp8&quot;&gt;Hopper GPU에서의 새로운 하드웨어 기능 - WGMMA, TMA, FP8 / New hardware features on Hopper GPUs - WGMMA, TMA, FP8&lt;/h2&gt;

&lt;p&gt;FlashAttention2가 Ampere(암페어, A100) GPU의 이론상 최대 FLOPS의 70%까지 달성할 수 있었지만, Hopper GPU의 새로운 기능을 최대한 활용하여 성능을 극대화하지 못했습니다. 여기에서는 새로운 Hopper-전용 기능 중 일부를 설명하고, 왜 이러한 기능이 중요한지를 설명하겠습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;While FlashAttention-2 can achieve up to 70% theoretical max FLOPS on Ampere (A100) GPUs, it does not yet take advantage of new features on Hopper GPUs to maximize performance. We describe some of the new Hopper-specific features here, and why they are important.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1. WGMMA(Warpgroup Matrix Multiply-Accumulate)는 Hopper의 새로운 텐서 코어(Tensor Core)를 활용하여 Ampere의 이전 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mma.sync&lt;/code&gt; 명령보다 훨씬 높은 처리량&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;을 제공합니다. (이미지는 &lt;a href=&quot;https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper?ncid=no-ncid&quot;&gt;H100 백서(whitepaper)&lt;/a&gt;에서 가져왔습니다.)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;1. WGMMA (Warpgroup Matrix Multiply-Accumulate). This new feature makes use of the new Tensor Cores on Hopper, with much higher throughput&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; than the older mma.sync instruction in Ampere (image from the &lt;a href=&quot;https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper?ncid=no-ncid&quot;&gt;H100 white paper&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg2.png&quot; alt=&quot;Hopper GPU에서의 새로운 기능: WGMMA(Warpgroup Matrix Multiply-Accumulate), H100 백서에서 가져온 이미지 / image from the H100 white paper&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2. TMA(Tensor Memory Accelerator)는 전역 메모리(Global Memory)와 공유 메모리(Shared Memory) 간의 데이터 전송을 가속화하는 특수 하드웨어 장치(special hardware unit)로, 모든 인덱스 연산과 범위-밖(out-of-bound)의 예측을 처리합니다. 이를 통해 레지스터를 확보할 수 있으며, 타일 크기와 효율성을 높이는 데 중요한 리소스를 제공합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;2. TMA (Tensor Memory Accelerator). This is a special hardware unit that accelerates the transfer of data between global memory and shared memory, taking care of all index calculation and out-of-bound predication. This frees up registers, which is a valuable resource to increase tile size and efficiency.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg3.png&quot; alt=&quot;Hopper GPU에서의 새로운 기능: TMA(Tensor Memory Accelerator) 소개 그림 / block diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3. FP8의 낮은 정밀도(Low-precision with FP8)에서의 연산은 FP16에서의 연산과 비교하여 텐서 코어 처리량(Tensor Core throughput)을 두 배로 높일 수 있습니다(예: FP16에서 989 TFLOPS, FP8에서 1978 TFLOPS). 그러나 더 적은 비트로 부동 소수점 수(floating point number)를 표현하기 때문에 정확도가 떨어집니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;3. Low-precision with FP8. This doubles the Tensor Core throughput (e.g. 989 TFLOPS with FP16 and 1978 TFLOPS with FP8), but trades off accuracy by using fewer bits to represent floating point numbers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg4.png&quot; alt=&quot;H100 GPU에서 FP8의 낮은 정밀도 사용 시 A100 FP16 대비 6배의 처리량 / 6x throughput&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FlashAttention-3는 &lt;a href=&quot;https://github.com/NVIDIA/cutlass&quot;&gt;NVIDIA의 CUTLASS&lt;/a&gt; 라이브러리의 강력한 추상화(abstraction)를 활용하여 Hopper의 이러한 기능들을 모두 활용합니다.&lt;br /&gt;
&lt;br /&gt;
이러한 새로운 기능들을 사용하도록 FlashAttention을 다시 작성함으로써 이미 FlashAttention-2 FP16 순전파(forward pass)에서 350 TFLOPS에서 540-570 TFLOPS로 성능을 크게 향상시킬 수 있습니다. 그러나 Hopper의 새로운 명령어(WGMMA 및 TMA)의 비동기적 특성(asynchronous nature)은 연산을 중첩(overlap)하고 성능을 더욱 향상시킬 수 있는 추가적인 알고리즘적 기회를 제공합니다. 이 블로그 글에서는 어텐션 연산에 특화된 두 가지 기법을 설명하겠습니다. 별도의 생성자(producer)와 소비자(consumer) 워프(warp)로 TMA와 WGMMA를 수행하는 워프 특수화(Warp Specialization) 기법은 GEMM의 문맥에서 &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization&quot;&gt;잘 설명해둔 것이 있으며&lt;/a&gt;,  여기에서도 동일하게 동작합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;FlashAttention-3 makes use of all of these new features of Hopper, using powerful abstractions from &lt;a href=&quot;https://github.com/NVIDIA/cutlass&quot;&gt;NVIDIA’s CUTLASS&lt;/a&gt; library. &lt;br /&gt;
&lt;br /&gt;
By rewriting FlashAttention to use these new features, we can already significantly speed it up (e.g., from 350 TFLOPS in FlashAttention-2 FP16 forward pass to around 540-570 TFLOPS). However, the asynchronous nature of the new instructions on Hopper (WGMMA and TMA) opens up additional algorithmic opportunities to overlap operations and thereby extract even greater performance. For this blogpost, we’ll explain two such techniques specific to attention. The generic technique of warp specialization, with separate producer and consumer warps doing TMA and WGMMA, is &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization&quot;&gt;well-covered elsewhere&lt;/a&gt; in the context of GEMM and works the same here.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;비동기성-gemm과-softmax-중첩하기--asynchrony-overlapping-gemm-and-softmax&quot;&gt;비동기성: GEMM과 Softmax 중첩하기 / Asynchrony: Overlapping GEMM and Softmax&lt;/h2&gt;

&lt;p&gt;왜 중첩을 해야 할까요?&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Why overlap?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;어텐션의 주요 연산은 GEMM(GEneral Matrix Multiplication / 일반적인 행렬 곱셈 연산, Q와 K 사이의 MatMul 연산 및 어텐션 확률 P와 V 사이의 matmul)과 Softmax입니다. 왜 중첩을 해야 할까요? 대부분의 FLOPS가 이미 GEMM에 있는데도요? GEMM이 (WGMMA 명령어를 사용하여 연산하여) 빠르다면, &lt;a href=&quot;https://horace.io/brrr_intro.html&quot;&gt;GPU가 부르르르 돌아가야&lt;/a&gt; 하지 않을까요?&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Attention has GEMMs (those matmuls between Q and K and between attention probability P and V) and softmax as its two main operations. Why do we need to overlap them? Isn’t most of the FLOPS in the GEMMs anyway? As long as the GEMMs are fast (e.g., computed using WGMMA instructions), shouldn’t the &lt;a href=&quot;https://horace.io/brrr_intro.html&quot;&gt;GPU be going brrrr&lt;/a&gt;?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;문제는 최신 가속기들에서 비-행렬곱(non-matmul) 연산이 행렬곱(matmul) 연산보다 매우 느리다는 것입니다. (Softmax의 경우) 지수(Exponential) 연산과 같은 특수 함수는 부동 소수점 곱셈보다 처리량이 매우 낮으며, 부동 소수점 곱하기-더하기(floating point multiply-add) 연산이나 행렬 곱하기-더하기(matrix multiply-add) 연산과 다른 장치(unit)인 다중 함수 장치(multi-function unit)에서 처리(evaluate)됩니다. 예를 들어, H100 GPU SXM5의 FP16 행렬곱 연산은 989 TPLOPS지만, 특수 함수&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;의 경우에는 (256배 적은 처리량인) 3.9 TFLOPS에 불과합니다! 헤드 차원(head dimension)이 128일 때, 지수 연산보다 행렬 연산의 FLOPS가 512배 더 많으며, 이는 지수 연산이 행렬 연산과 비교했을 때 절반 가량의 시간이 소요될 수 있음을 뜻합니다. FP8의 경우 상황은 더욱 악화되어 행렬곱(matmul) 연산 FLOPS는 2배 빠르지만 지수 연산 FLOPS는 동일한 속도를 유지합니다. 이상적으로는 행렬곱(matmul)과 Softmax가 병렬로 동작하는 것이 좋습니다. 텐서 코어(Tensor Core)가 행렬곱 연산으로 바쁜 사이에 다중 함수 장치는 지수 연산을 처리해야 합니다!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The problem is that non-matmul operations are much slower than matmul operations on modern accelerators. Special functions such as exponential (for the softmax) have even lower throughput than floating point multiply-add; they are evaluated by the multi-function unit, a unit separate from floating point multiply-add or matrix multiply-add. As an example, the H100 GPU SXM5 has 989 TFLOPS of FP16 matrix multiply, but only 3.9 TFLOPS (256x less throughput) for special functions&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;! For head dimension 128, there are 512x more matmul FLOPS than exponential, which means that exponential can take 50% of the time compared to matmul. The situation is even worse for FP8, where the matmul FLOPS are twice as fast yet exponential FLOPS stay the same speed. Ideally we want matmul and softmax to operate in parallel. While the Tensor Cores are busy with matmul, the multi-function units should be calculating exponential!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;핑퐁-스케쥴링으로-워프그룹-간-중첩--inter-warpgroup-overlapping-with-pingpong-scheduling&quot;&gt;핑퐁 스케쥴링으로 워프그룹 간 중첩 / Inter-warpgroup overlapping with pingpong scheduling&lt;/h3&gt;

&lt;p&gt;GEMM과 Softmax를 중첩하는 가장 쉬운 첫번째 방법은 아무것도 하지 않는 것입니다! 워프 스케줄러(Warp Scheduler)는 이미 일부 워프(warp)가 차단되는 경우(예. GEMM 결과를 기다리는 경우) 다른 워프(warp)가 실행될 수 있도록 스케줄링합니다. 즉, 아무것도 하지 않아도 워프 스케줄러는 이미 일부 중첩을 수행하고 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The first and easiest way to overlap GEMM and softmax is to do nothing at all! The warp schedulers already try to schedule warps so that if some warps are blocked (e.g., waiting for GEMM results), other warps can run. That is, the warp schedulers do some of this overlapping for us, for free.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;하지만 일부 스케줄링을 수동으로 수행하여 이를 개선할 수 있습니다. 예를 들어 아래 그림의 Warpgroup 1과 2처럼 (각 4개의 워프로 구성된) 2개의 워프그룹(warpgroup)이 있을 때, 동기화 배리어(synchronization barrier, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bar.sync&lt;/code&gt;)를 사용하여 워프그룹1이 먼저 GEMM 연산들(예. 첫번째 반복의 GEMM1과 그 다음 반복의 GEMM0)을 수행한 다음, 워프그룹1이 Softmax 연산을 수행하는 동안 워프그룹2가 GEMM 연산을 수행하도록 할 수 있습니다. 이러한 “핑퐁(pingpong)” 스케줄은 아래 그림에 설명되어 있으며, 동일한 색상은 동일한 반복(iteration)을 나타냅니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;However, we can improve on this by doing some of the scheduling manually. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. This “pingpong” schedule is illustrated in the figure below, where the same color denotes the same iteration.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg5.png&quot; alt=&quot;핑퐁 스케쥴링으로 워프그룹 간 중첩 설명 그림 / block chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 하면 다른 워프그룹의 GEMM 연산의 그림자 안에서 Softmax를 수행할 수 있습니다. 물론 이 그림은 대략적인 것으로, 실제 스케줄링은 이렇게 깔끔하지 않습니다. 그러나 핑퐁 스케줄링을 사용하면 (헤드 차원이 128이고 시퀀스 길이가 8K인 경우에) FP16 어텐션의 순전파 성능을 약 570 TFLOPS에서 620 TFLOPS로 향상시킬 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This would allow us to perform the softmax in the shadow of the GEMMs of the other warpgroup. Of course, this figure is just a caricature; in practice the scheduling is not really this clean. Nevertheless, pingpong scheduling can improve FP16 attention forward pass from around 570 TFLOPS to 620 TFLOPS (head dim 128, seqlen 8K).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;gemm-및-softmax-연산의-워프그룹-내-중첩--intra-warpgroup-overlapping-of-gemm-and-softmax&quot;&gt;GEMM 및 Softmax 연산의 워프그룹 내 중첩 / Intra-warpgroup overlapping of GEMM and Softmax&lt;/h3&gt;

&lt;p&gt;하나의 워프그룹 내에서도 GEMM과 Softmax의 일부를 중첩할 수 있습니다. 아래 그림에서와 같이, 동일한 색상은 동일한 반복(iteration)을 나타냅니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Even within one warpgroup, we can have some part of softmax running while the GEMMs of that warpgroup is running. This is illustrated in this figure, where the same color denotes the same iteration.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg6.png&quot; alt=&quot;GEMM 및 Softmax 연산의 워프그룹 내 중첩 설명 그림 / block chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 파이프라인은 FP16 어텐션 순전파 시 처리량을 620TFLOPS에서 640-660TFLOPS로 증가시키지만 레지스터 압력(register pressure)이 높아지는 대가를 치릅니다. GEMM의 누산기(accumulator)와 Softmax의 입/출력을 모두 수용하려면 더 많은 레지스터가 필요합니다. 전체적으로 이 기법이 더 나은 트레이드-오프(favorable trade-off)를 제공한다는 것을 알 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This pipelining increases throughput from around 620 TFLOPS to around 640-660 TFLOPS for FP16 attention forward, at the cost of higher register pressure. We need more registers to hold both accumulators of the GEMMs, and the input/output of softmax. Overall, we find this technique to offer a favorable tradeoff.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;낮은-정밀도-비일관적-처리로-양자화-오차-줄이기--low-precision-reduce-quantization-error-with-incoherent-processing&quot;&gt;낮은 정밀도: 비일관적 처리로 양자화 오차 줄이기 / Low-precision: reduce quantization error with incoherent processing&lt;/h2&gt;

&lt;p&gt;LLM 활성화(activation)에는 다른 특징(feature)들보다 훨씬 큰 크기의 &lt;a href=&quot;https://arxiv.org/abs/2208.07339&quot;&gt;이상치(outlier)들&lt;/a&gt;이 있을 수 있습니다. 이러한 이상치는 훨씬 큰 양자화 오류(quantization error)를 만들어서 양자화(quantize)를 어렵게 합니다. 양자화 분야(quantization literature)에서 사용하는, Query와 Key에 임의의 직교 행렬(random orthogonal matrix)를 곱하여 이상치를 “퍼뜨려(spread out)” 양자화 오류를 줄이는 비일관적 처리(incoherent processing)를 활용합니다. (예. &lt;a href=&quot;https://arxiv.org/abs/2307.13304&quot;&gt;QuIP&lt;/a&gt;) 그 중에서도 (임의의 부호를 사용하는) 하다마드 변환(Hadamad transform)을 사용하는데, 이 방법으로 헤드 차원 $d$에 대해서 어텐션 헤드당 $O(d^2)$ 대신 $O(dlogd)$ 시간에 수행할 수 있습니다. 하다마드 변환은 메모리 대역폭(memory-bandwidth)에 제한이 있으므로, (역시 메모리 대역폭에 제한이 있는) 로터리 임베딩(rotary embedding)과 같은 이전 연산과 “곧바로(for free)” 병합(fuse)할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;LLM activation can have &lt;a href=&quot;https://arxiv.org/abs/2208.07339&quot;&gt;outliers&lt;/a&gt; with much larger magnitude than the rest of the features. These outliers make it difficult to quantize, producing much larger quantization errors. We leverage incoherent processing, a technique used in the quantization literature (e.g. from &lt;a href=&quot;https://arxiv.org/abs/2307.13304&quot;&gt;QuIP&lt;/a&gt;) that multiplies the query and key with a random orthogonal matrix to “spread out” the outliers and reduce quantization error. In particular, we use the Hadamard transform (with random signs), which can be done per attention head in $O(d logd)$ instead of $O(d^2)$ time, where $d$ is the head dimension. Since the Hadamard transform is memory-bandwidth bound, it can be fused with previous operations such as rotary embedding (also memory-bandwidth bound) “for free”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;실험을 통해 표준 정규 분포(Standard Normal Distribution)에서 Q, K, V를 생성하면서 (이상치를 나타내기 위해) 0.1%의 항목들이 큰 값을 가지도록 한 경우, 비일관적 처리 시에 양자화 오류를 2.6배 낮출 수 있음을 확인하였습니다. 아래 표에서 수치 오류 비교해두었습니다. 자세한 내용은 논문을 참고해주세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In our experiment where Q, K, V are generated from a standard normal distribution but 0.1% of the entries have large magnitudes (to simulate outliers), we found that incoherent processing can reduce the quantization error by 2.6x. We show numerical error comparison in the table below. Please see the paper for details.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg6a.png&quot; alt=&quot;FP8의 낮은 정밀도에서 비일관적 처리 시의 양자화 오류 비교표 / text diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;어텐션-벤치마크--attention-benchmark&quot;&gt;어텐션 벤치마크 / Attention benchmark&lt;/h2&gt;

&lt;p&gt;FlashAttention-3 사용 시의 결과를 FlashAttention-2 및 (이미 Hopper GPU의 새로운 하드웨어 기능을 사용 중인) Triton, cuDNN의 구현과 비교해보겠습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We show some results with FlashAttention-3, and compare it to FlashAttention-2, as well as the implementation in Triton and cuDNN (both of which already use new hardware features of Hopper GPUs).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;FP16에서, FlashAttention-2 대비 1.6-1.8배의 속도 개선을 확인할 수 있었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For FP16, we see about 1.6x-1.8x speedup over FlashAttention-2&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg7.png&quot; alt=&quot;FlashAttention-3의 순전파시 속도 개선 비교: FlashAttention-2, Triton, cuDNN과의 비교 차트 / speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg8.png&quot; alt=&quot;FlashAttention-3의 역전파시 속도 개선 비교: FlashAttention-2, Triton, cuDNN과의 비교 차트 / speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FP8에서는 1.2PFLOPS(PetaFLOPS)에 가까운 속도를 보였습니다!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For FP8, we can reach close to 1.2 PFLOPS!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg9.png&quot; alt=&quot;FlashAttention-3의 FP8에서의 속도 개선 비교: Triton, cuDNN과의 비교 차트 / speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;토의--discussion&quot;&gt;토의 / Discussion&lt;/h2&gt;

&lt;p&gt;이번 블로그 글은 Hopper GPU에서 사용할 수 있는 FlashAttention의 최적화 기능 중 일부를 중점적으로 설명했습니다. 가변 길이 시퀀스(variable length sequence)나 영속적인 커널(persistent kernel), FP8에서의 커널 내 전치(in-kernel transpose for FP8)과 같은 다른 최적화들에 대해서는 논문에서 다루었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This blogpost highlights some of the optimizations for FlashAttention available on Hopper GPUs. Other optimizations (e.g., variable length sequences, persistent kernel, and in-kernel transpose for FP8) are covered in the paper.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;지금까지 실행되는 하드웨어를 활용하는 알고리즘을 설계하여 효율성을 크게 향상시키고 긴 컨텍스트와 같은 새로운 모델 기능을 활용할 수 있음을 확인하였습니다. 향후 LLM 추론에서의 최적화 작업과 같은 추가 연구를 기대하며, 이러한 기법이 일반화되어 다른 하드웨어 구조에도 적용될 수 있기를 기대합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We have seen that designing algorithms that take advantage of the hardware they run on can bring significant efficiency gains and unlock new model capabilities such as long context. We look forward to future work on optimization for LLM inference, as well as generalizing our techniques to other hardware architectures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또한 향후 PyTorch의 릴리즈에 FlashAttention-3가 반영되기를 기대합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We also look forward to FlashAttention-3 being integrated in a future release of PyTorch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2 id=&quot;각주--notes&quot;&gt;각주 / Notes&lt;/h2&gt;

&lt;!-- MathJAX 설정 추가 --&gt;
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
&lt;!--
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: &quot;AMS&quot;
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook(&quot;Math Processing Error&quot;,function (message) {
     alert(&quot;Math Processing Error: &quot;+message[1]);
});
MathJax.Hub.Register.MessageHook(&quot;TeX Jax - parse error&quot;,function (message) {
     alert(&quot;Math Processing Error: &quot;+message[1]);
});
--&gt;
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;WGMMA 명령어가 없는 경우, 이전의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mma.sync&lt;/code&gt; 명령어는 Hopper 텐서 코어 최대 처리량의 약 2/3에 불과합니다: &lt;a href=&quot;https://arxiv.org/abs/2402.13499v1&quot;&gt;https://arxiv.org/abs/2402.13499v1&lt;/a&gt;&lt;/p&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Without the wgmma instruction, the older mma.sync instruction can only reach about ⅔ the peak throughput of Hopper Tensor Cores: https://arxiv.org/abs/2402.13499v1&lt;/p&gt;
      &lt;/blockquote&gt;
      &lt;p&gt;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;CUDA 프로그래밍 가이드에 따르면 특수 함수의 처리량은 각 스트리밍 멀티프로세서(SM)의 클럭 주기(clock cycle)당 16개의 연산입니다. 스트리밍 멀티프로세서(SM) 132개에 16을 곱하고 (FP16 행렬곱의 989 TFLOPS를 클럭 속도(clock speed)로 계산한) 1830 Mhz를 곱하여 3.9TFLOPS를 얻습니다.&lt;/p&gt;
      &lt;blockquote&gt;
        &lt;p&gt;The CUDA programming guide specifies that the throughput for special functions is 16 operations per streaming multiprocessor (SM) per clock cycle. We multiply 16 by 132 SMs and 1830 Mhz (clock speed used to calculate 989 TFLOPS of FP16 matmul) to get 3.9 TFLOPS&lt;/p&gt;
      &lt;/blockquote&gt;
      &lt;p&gt;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Jay Shah and Ganesh Bikshandi, Colfax Research, Ying Zhang, Meta, Vijay Thakkar and Pradeep Ramani, NVIDIA, Tri Dao, TogetherAI and Princeton University</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">어텐션(Attention)은 트랜스포머(Transformer) 구조의 핵심 계층(layer)이지만, 대규모 언어 모델(LLM, Large Language Model)과 긴-컨텍스트 애플리케이션(long-context application)의 병목(bottleneck)이기도 합니다. FlashAttention (및 FlashAttention-2)은 메모리 읽기/쓰기를 최소화하여 GPU에서 어텐션 연산을 가속화하는 방법을 개척했으며, 이제 대부분의 라이브러리에서 트랜스포머(Transformer) 학습 및 추론을 가속화하는데 사용되고 있습니다. 이 덕분에 지난 2년 동안 LLM 컨텍스트 길이가 2-4K(GPT-3, OPT)부터 128K(GPT-4) 및 1M(Llama 3)까지 급격히 할 수 있었습니다. 그러나 이러한 성공에도 불구하고, FlashAttention(플래시어텐션)은 최신 하드웨어의 새로운 기능을 아직 활용하지 못했습니다. FlashAttention-2는 H100 GPU에서 이론적 최대 FLOPS(FLoating-point Operations Per Second)의 35%만 활용했습니다. 이 블로그 글에서는 Hopper(호퍼, H100) GPU에서 어텐션 연산을 가속화하기 위한 세 가지 주요 기술을 설명합니다: (1) 워프 특수화(warp-specialization)를 통해 전체 연산과 데이터 이동을 중첩(overlap)하고, (2) 블록-단위 MatMul(Block-wise MatMul) 및 Softmax 연산을 교차로 수행하며, (3) 저정밀도(low-precision) FP8을 위한 하드웨어 지원을 활용하는 비일관적 처리(incoherent processing)입니다. Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most libraries to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (Llama 3). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.</summary></entry><entry><title type="html">PyTorch로 전문가 혼합(MoE) 모델 학습 확장하기</title><link href="https://pytorch.kr/blog/2024/training-moes/" rel="alternate" type="text/html" title="PyTorch로 전문가 혼합(MoE) 모델 학습 확장하기" /><published>2024-06-23T00:00:00+09:00</published><updated>2024-06-23T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2024/training-moes</id><content type="html" xml:base="https://pytorch.kr/blog/2024/training-moes/">&lt;p&gt;최근 1년간 전문가 혼합(MoE, Mixture-of-Experts) 모델들의 인기가 급증했습니다. 이러한 인기는 &lt;a href=&quot;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&quot;&gt;DBRX&lt;/a&gt;, &lt;a href=&quot;https://mistral.ai/news/mixtral-of-experts/&quot;&gt;Mixtral&lt;/a&gt;, &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2&quot;&gt;DeepSeek&lt;/a&gt;를 비롯하여 다양하고 강력한 오픈소스 모델들로부터 비롯된 것입니다. Databricks에서는 PyTorch 팀과 협력하여 MoE 모델의 학습을 확장했습니다. 이번 글에서는 &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch Distributed&lt;/a&gt; 및 PyTorch로 구현한 효율적인 오픈소스 MoE 구현체인 &lt;a href=&quot;https://github.com/databricks/megablocks&quot;&gt;MegaBlocks&lt;/a&gt;를 사용하여 학습을 3천개 이상의 GPU들로 확장하는 방법에 대해 이야기해보겠습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Over the past year, Mixture of Experts (MoE) models have surged in popularity, fueled by powerful open-source models like &lt;a href=&quot;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&quot;&gt;DBRX&lt;/a&gt;, &lt;a href=&quot;https://mistral.ai/news/mixtral-of-experts/&quot;&gt;Mixtral&lt;/a&gt;, &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2&quot;&gt;DeepSeek&lt;/a&gt;, and many more. At Databricks, we’ve worked closely with the PyTorch team to scale training of MoE models. In this blog post, we’ll talk about how we scale to over three thousand GPUs using &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch Distributed&lt;/a&gt; and &lt;a href=&quot;https://github.com/databricks/megablocks&quot;&gt;MegaBlocks&lt;/a&gt;, an efficient open-source MoE implementation in PyTorch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;전문가-혼합moe이-무엇인가요--what-is-a-moe&quot;&gt;전문가 혼합(MoE)이 무엇인가요? / What is a MoE?&lt;/h2&gt;

&lt;p&gt;전문가 혼합(MoE, Mixture-of-Experts) 모델은 여러 전문가 네트워크들을 사용하여 예측을 수행하는 모델 구조입니다. 게이팅(gating) 네트워크는 전문가 네트워크들의 출력을 라우팅하고 결합하는데 사용하며, 각 전문가가 서로 다른 토큰들의 분포(specialized distribution of tokens)로 학습되도록 합니다. 트랜스포머 기반의 대규모 언어 모델(LLM, Large Language Model)은 일반적으로 임베딩 레이어 뒤에 여러 개의 트랜스포머 블록들로 구성됩니다. (그림 1, 제일 왼쪽 A) 각 트랜스포머 블록에는 어텐션 블록(attention block)과 덴스 피드 포워드 네트워크(dense feed forward network)가 포함되어 있습니다. (그림 1, 왼쪽 두번째 B) 이러한 트랜스포머 블록들은 하나의 블록의 출력이 다음 블록의 입력으로 이어지도록 쌓여 있습니다. 최종 출력은 완전 연결된 레이어(fully connected layer)와 소프트맥스(softmax)를 거쳐 다음에 출력할 토큰에 대한 확률을 얻습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A MoE model is a model architecture that uses multiple expert networks to make predictions. A gating network is used to route and combine the outputs of experts, ensuring each expert is trained on a different, specialized distribution of tokens. The architecture of a transformer-based large language model typically consists of an embedding layer that leads into multiple transformer blocks (Figure 1, Subfigure A). Each transformer block contains an attention block and a dense feed forward network (Figure 1, Subfigure B). These transformer blocks are stacked such that the output of one transformer block leads to the input of the next block. The final output goes through a fully connected layer and softmax to obtain probabilities for the next token to output.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;LLM에 MoE를 적용할 때는, 덴스 피드 포워드 레이어(dense feed forward layer)가 MoE 레이어로 대체됩니다. MoE 레이어는 게이팅 네트워크와 여러 전문가들로 구성됩니다. (그림 1, 오른쪽 D) 게이팅 네트워크는 일반적으로 선형 피드 포워드 네트워크(linear feed forward network)로, 각 토큰을 받아 어떤 토큰이 어떤 전문가로 라우팅되어야 하는지 결정하도록 하는 가중치 세트(set of weights)를 생성합니다. 전문가 네트워크들 자체도 일반적으로 피드 포워드 네트워크로 구현합니다. 학습 중에는 게이팅 네트워크는 입력을 전문가들에게 할당하여, 모델이 특화되고 성능이 향상되도록 합니다. 이후, 라우터 출력을 전문가 출력을 더하여(weigh) MoE 레이어의 최종 출력으로 제공합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;When using a MoE in LLMs, the dense feed forward layer is replaced by a MoE layer which consists of a gating network and a number of experts (Figure 1, Subfigure D). The gating network, typically a linear feed forward network, takes in each token and produces a set of weights that determine which tokens are routed to which experts. The experts themselves are typically implemented as a feed forward network as well. During training, the gating network adapts to assign inputs to the experts, enabling the model to specialize and improve its performance. The router outputs are then used to weigh expert outputs to give the final output of the MoE layer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg1.png&quot; alt=&quot;그림 1: 트랜스포머 블록에서 전문가 혼합(MoE) 사용하기&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;그림 1: 트랜스포머 블록에서 전문가 혼합(MoE) 사용하기 / Figure 1: Using Mixture of Experts in a transformer block&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;더 큰 밀집된 모델(dense model)들에 비해, MoE 모델은 주어진 연산 한도(compute budget)로 더 효율적으로 학습할 수 있습니다. 이는 게이팅 네트워크가 토큰을 전문가들 중 일부에게만 보내어 연산 부하를 줄이기 때문입니다. 결과적으로, 모델의 용량(= 전체 매개변수의 수)을 늘리면서도 이에 비례하여 연산 요구 사항(computational requirements)을 늘리지 않아도 됩니다. 추론 시에는 전문가들 중 일부만 사용하므로 MoE는 더 큰 밀집된 모델(dense model)에 비해 더 빠르게 추론을 수행할 수 있습니다. 그러나, 메모리에는 사용 중인 전문가들만이 아니라 전체 모델을 불러와야(loaded) 합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Compared to dense models, MoEs provide more efficient training for a given compute budget. This is because the gating network only sends tokens to a subset of experts, reducing the computational load. As a result, the capacity of a model (its total number of parameters) can be increased without proportionally increasing the computational requirements. During inference, only some of the experts are used, so a MoE is able to perform faster inference than a dense model. However, the entire model needs to be loaded in memory, not just the experts being used.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;MoE의 희소성(sparsity)은 특정 토큰이 일부 전문가들에게만 라우팅되도록 하여 연산 효율을 높여줍니다. 전문가의 수와 전문가를 선택하는 방법은 게이팅 네트워크의 구현에 따라 다르지만, 상위 k개(top-k)가 일반적인 방법입니다. 게이팅 네트워크는 먼저 각 전문가들에 대한 확률 값을 예측한 다음, 상위 k개의 전문가들(top k experts)에게 토큰을 전달(route)하여 출력을 얻습니다. 하지만, 모든 토큰들이 항상 동일한 일부 전문가들에게만 전달되면, 학습이 비효율적으로 되고 다른 전문가들은 학습이 잘 되지 않게 됩니다. 이러한 문제를 완화하기 위해 모든 전문가들에게 고르게(even) 라우팅되도록 하는 로드 밸런싱 손실(load balancing loss)이 도입되었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The sparsity in MoEs that allows for greater computational efficiency comes from the fact that a particular token will only be routed to a subset of experts. The number of experts and how experts are chosen depends on the implementation of the gating network, but a common method is top k. The gating network first predicts a probability value for each expert, then routes the token to the top k experts to obtain the output. However, if all tokens always go to the same subset of experts, training becomes inefficient and the other experts end up undertrained. To alleviate this problem, a load balancing loss is introduced that encourages even routing to all experts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;전문가의 수와 상위 k개의 전문가를 고르는 것은 MoE 모델 설계 시의 중요한 요소입니다. 전문가의 수가 많을수록 연산 비용을 늘리지 않으면서도 더 큰 모델로 확장할 수 있습니다. 이는 모델이 더 많은 학습을 할 수 있는 능력(capacity)을 갖춤을 뜻하지만, 일정 수준 이상으로 전문가의 수를 늘리면 성능 향상이 줄어드는(diminish) 경향이 있습니다. 전체 모델을 메모리에 불러와야 하므로 몇 개의 전문가를 선택할지는 모델 서빙 시의 추론 비용과 균형을 맞춰야 합니다. 상위 k개(top-k)를 선택할 때도 마찬가지로, 학습 중에 더 작은 k개를 선택하면 행렬 곱 연산(matrix multiplication)을 적게 수행하게 되어, 통신 비용이 큰 경우 연산 자원이 남게(leaving free computation on the table) 됩니다. 하지만 더 큰 k개를 선택하면 추론 속도가 일반적으로 느려지게 됩니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The number of experts and choosing the top k experts is an important factor in designing MoEs. A higher number of experts allows scaling up to larger models without increasing computational cost. This means that the model has a higher capacity for learning, however, past a certain point the performance gains tend to diminish. The number of experts chosen needs to be balanced with the inference costs of serving the model since the entire model needs to be loaded in memory. Similarly, when choosing top k, a lower top k during training results in smaller matrix multiplications, leaving free computation on the table if communication costs are large enough. During inference, however, a higher top k generally leads to slower inference speed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;메가블록--megablocks&quot;&gt;메가블록 / MegaBlocks&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/databricks/megablocks&quot;&gt;MegaBlocks&lt;/a&gt;은 희소 행렬 곱(sparse matrix multiplication)을 사용하여 토큰 할당이 불균형(uneven)하더라도 전문가 출력을 병렬로 연산하는 효율적인 MoE 구현체입니다. MegaBlocks는 GPU 커널을 사용하는 동안 토큰을 버리지 않으므로(avoid dropping tokens) 효율적인 학습을 유지하는 Dropless MoE를 구현합니다. MegaBlocks 이전에는 연산 시 토큰을 버리거나 패딩(padding)에 연산 자원과 메모리를 낭비하는 등, 모델 품질(model quality)과 하드웨어 효율성(hardware efficiency) 사이에서 절충점(trade-offs)을 찾아야 하는 동적 라우팅 공식(dynamic routing dormulation)을 사용했습니다. 전문가 네트워크들은 다양한 수의 토큰들(variable number of tokens)을 받을 수 있으며, 전문가 연산(expert computation)은 블록 희소 행렬 곱(block sparse matrix multiplication)을 사용하여 효율적으로 수행할 수 있습니다. 우리는 &lt;a href=&quot;https://www.databricks.com/blog/bringing-megablocks-databricks&quot;&gt;LLM Foundry에 MegaBlocks를 통합(integrate)&lt;/a&gt;하여 MoE 학습을 수천개의 GPU로 확장할 수 있도록 했습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://github.com/databricks/megablocks&quot;&gt;MegaBlocks&lt;/a&gt; is an efficient MoE implementation that uses sparse matrix multiplication to compute expert outputs in parallel despite uneven token assignment. MegaBlocks implements a dropless MoE that avoids dropping tokens while using GPU kernels that maintain efficient training. Prior to MegaBlocks, dynamic routing formulations forced a tradeoff between model quality and hardware efficiency. Previously, users had to either drop tokens from computation or waste computation and memory on padding. Experts can receive a variable number of tokens and the expert computation can be performed efficiently using block sparse matrix multiplication. We’ve &lt;a href=&quot;https://www.databricks.com/blog/bringing-megablocks-databricks&quot;&gt;integrated MegaBlocks into LLM Foundry&lt;/a&gt; to enable scaling MoE training to thousands of GPUs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg2.png&quot; alt=&quot;그림 2: 전문가 연산(expert computation) 시의 행렬 곱 연산&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;그림 2: 전문가 연산 시의 행렬 곱 연산 / Figure 2: Matrix multiplication for expert computations&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;전문가-병렬화--expert-parallelism&quot;&gt;전문가 병렬화 / Expert Parallelism&lt;/h3&gt;

&lt;p&gt;모델이 더 큰 크기로 확장되어 하나의 GPU에 올라가지 않는다면, 더 고급 형태의 병렬 처리(advanced forms of parallelism)가 필요합니다. 전문가 병렬화(Expert Parallelism)는 모델 병렬화(Model Parallelism)의 일종으로, 성능 향상을 위해 서로 다른 GPU에 서로 다른 전문가를 배치하는 형태입니다. 전문가 네트워크의 가중치들을 모든 GPU들 간에 공유(communicate)하는 대신, 토큰들이 각 전문가를 포함하고 있는 장치로 전송됩니다. 가중치 대신 데이터를 이동함으로써, 여러 기기들(multiple machines)에서 단일 전문가 네트워크를 위한 데이터를 집계(aggregate)할 수 있습니다. 라우터(router)는 입력 시퀀스(input sequence)에서 어떤 토큰을 어떠한 전문가에게 보낼지를 결정합니다. 이는 일반적으로 각 토큰-전문가 쌍(token-expert pair)에서 게이팅 점수(gating score)를 계산한 다음, 각 토큰을 최고 점수를 받은 전문가쪽으로 전달(route)하는 식으로 이뤄집니다. 토큰별 전문가 할당(token-to-expert assignment)이 결정되고 나면, 전체-대-전체(all-to-all) 통신 단계를 수행하여 해당 전문가를 호스팅하는 장치로 토큰을 전송합니다. 이 단계에는 각 디바이스들이 해당 장치의 전문가에게 할당된 토큰을 받는 동시에 다른 디바이스의 전문가에게 할당된 토큰을 보내는 과정이 포함됩니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;As models scale to larger sizes and fail to fit on a single GPU, we require more advanced forms of parallelism. Expert parallelism is a form of model parallelism where we place different experts on different GPUs for better performance. Instead of expert weights being communicated across all GPUs, tokens are sent to the device that contains the expert. By moving data instead of weights, we can aggregate data across multiple machines for a single expert. The router determines which tokens from the input sequence should be sent to which experts. This is typically done by computing a gating score for each token-expert pair, and then routing each token to the top-scoring experts. Once the token-to-expert assignments are determined, an all-to-all communication step is performed to dispatch the tokens to the devices hosting the relevant experts. This involves each device sending the tokens assigned to experts on other devices, while receiving tokens assigned to its local experts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;전문가 병렬화의 주요 장점은 여러 개의 작은 행렬 곱셈(matrix multiplication) 대신, 몇 개의 더 큰 행렬 곱셈을 처리할 수 있다는 것입니다. 각 GPU는 전문가의 일부만을 가지고 있기 때문에, 해당 전문가에 대한 연산만 수행하면 됩니다. 따라서 여러 GPU들 간의 토큰을 집계하면 각 행렬의 크기도 비례해서 커집니다. GPU는 대규모 병렬 연산에 최적화되어 있으므로, 대규모 작업일수록 그러한 기능들을 더 잘 활용할 수 있어 활용도(utilization)와 효율성(efficiency)이 높아집니다. 더 큰 행렬 곱셈의 이점에 대한 보다 자세한 설명은 &lt;a href=&quot;https://www.thonking.ai/p/what-shapes-do-matrix-multiplications&quot;&gt;여기&lt;/a&gt;에서 확인할 수 있습니다. 연산이 완료되고 나면 전문가의 출력을 원래 장치로 보내기 위해 다시 전체-대-전체(all-to-all) 통신 단계가 수행됩니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The key advantage of expert parallelism is processing a few, larger matrix multiplications instead of several small matrix multiplications. As each GPU only has a subset of experts, it only has to do computation for those experts. Correspondly, as we aggregate tokens across multiple GPUs, the size of each matrix is proportionally larger. As GPUs are optimized for large-scale parallel computations, larger operations can better exploit their capabilities, leading to higher utilization and efficiency. A more in depth explanation of the benefits of larger matrix multiplications can be found &lt;a href=&quot;https://www.thonking.ai/p/what-shapes-do-matrix-multiplications&quot;&gt;here&lt;/a&gt;. Once the computation is complete, another all-to-all communication step is performed to send the expert outputs back to their original devices.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg3.png&quot; alt=&quot;그림 3: 전문가 병렬화에서의 토큰 라우팅&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;그림 3: 전문가 병렬화에서의 토큰 라우팅 / Figure 3: Token routing in expert parallelism&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;우리는 텐서가 어떻게 샤딩(shard)되고 복제(replicate)되는지를 설명하는 저수준(low-level)의 추상화된 PyTorch의 &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md&quot;&gt;DTensor&lt;/a&gt;를 활용하여 전문가 병렬화를 효과적으로 구현하였습니다. 먼저 전문가를 서로 다른 GPU들에 수동으로 배치한 뒤, 노드 전체에 걸쳐 샤딩하여 토큰 라우팅 시에 빠른 GPU 통신을 위해 NVLink를 활용할 수 있도록 합니다. 그런 다음 전체 클러스터에 걸쳐 병렬화를 간결하게(succinctly) 설명할 수 있는 &lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_device_mesh.html&quot;&gt;디바이스 메쉬(device mesh)&lt;/a&gt;를 이러한 구성(layout) 위에 구축할 수 있습니다. 디바이스 메쉬를 사용하여 다른 형태의 병렬화(alternate forms of parallelism)에 필요한 쉽게 전문가들을 저장(checkpoint)하거나 재배치(rearrange)할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We leverage PyTorch’s &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md&quot;&gt;DTensor&lt;/a&gt;, a low-level abstraction for describing how tensors are sharded and replicated, to effectively implement expert parallelism. We first manually place experts on different GPUs, typically sharding across a node to ensure we can leverage NVLink for fast GPU communication when we route tokens. We can then build a &lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_device_mesh.html&quot;&gt;device mesh&lt;/a&gt; on top of this layout, which lets us succinctly describe the parallelism across the entire cluster. We can use this device mesh to easily checkpoint or rearrange experts when we need alternate forms of parallelism.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;pytorch-fsdp로-zero-3-확장하기--scaling-zero-3-with-pytorch-fsdp&quot;&gt;PyTorch FSDP로 ZeRO-3 확장하기 / Scaling ZeRO-3 with PyTorch FSDP&lt;/h3&gt;

&lt;p&gt;전문가 병렬화와 결합하여, 다른 모든 레이어들에 대해 데이터 병렬화(Data Parallelism)을 사용합니다. 각 GPU에 모델과 옵티마이저(optimizer)의 복사본을 저장하고 데이터의 서로 다른 부분(chunk)을 처리합니다. 각 GPU가 순전파(forward) 및 역전파(backward)를 완료한 뒤, 전체 모델(global model)의 업데이트를 위해 GPU들에서 변화도(gradient)를 집계(accumulate)합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In conjunction with expert parallelism, we use data parallelism for all other layers, where each GPU stores a copy of the model and optimizer and processes a different chunk of data. After each GPU has completed a forward and backward pass, gradients are accumulated across GPUs for a global model update.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ZeRO-3는 가중치(weight)와 옵티마이저(optimizer)를 각 GPU에 복제하는 대신 분산(shard)하는 데이터 병렬화의 한 형태입니다. 이렇게 하면 각 GPU에는 전체 모델의 일부만 저장하므로 메모리의 부담(memory pressure)를 극적(dramatically)으로 줄일 수 있습니다. 연산 시 모델의 일부가 필요할 때는 다른 GPU들로부터 수집한 다음, 연산이 완료되면 수집했던 가중치를 제거(discard)합니다. 우리는 ZeRO-3의 PyTorch 구현인 &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/&quot;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt;를 사용합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ZeRO-3 is a form of data parallelism where weights and optimizers are sharded across each GPU instead of being replicated. Each GPU now only stores a subset of the full model, dramatically reducing memory pressure. When a part of the model is needed for computation, it is gathered across all the GPUs, and after the computation is complete, the gathered weights are discarded. We use PyTorch’s implementation of ZeRO-3, called &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/&quot;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;수천개의 GPU로 확장 시에는 장치들 간의 통신 비용이 증가하여 학습 속도가 느려지게 됩니다. 통신이 증가하는 것은 모든 GPU들 간에 모델 매개변수와 변화도(gradient), 옵티마이저 상태(Optimizer state)들을 동기화하고 공유하기 때문이며, 여기에는 올게더(all-gather) 및 리듀스-스캐터(reduce-scatter) 연산이 포함됩니다. 이러한 문제를 완화하는 동시에 FSDP의 이점을 유지하기 위해 우리는 전문가 병렬화와 결합하여 Hybrid Sharded Data Parallel (HSDP)을 사용하여 모델과 옵티마이저를 일정한 개수의 GPU들의 묶음(a set number of GPUs)에 분산한 뒤, 이를 여러번 복제하여 클러스터를 완전히 활용할 수 있도록 합니다. HSDP를 사용하면 모든 복제본(replica)들 간의 변화도(gradient)를 동기화하기 위해 역전파 단계에서 올-리듀스(all-reduce) 연산이 추가로 필요합니다. 이 접근법을 사용하여 대규모 분산 학습 시에 메모리 효율성과 통신 비용 간의 균형을 맞출 수 있습니다. HSDP를 사용하기 위해서는 이전의 전문가 병렬화에서의 디바이스 메쉬(device mesh)를 확장하여 필요할 때 실제로 분산(shard)과 수집(gather)의 무거운 작업을 PyTorch가 수행하도록 합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;As we scale to thousands of GPUs, the cost of communication across devices increases, slowing down training. Communication increases due to the need to synchronize and share model parameters, gradients, and optimizer states across all GPUs which involves all-gather and reduce-scatter operations. To mitigate this issue while keeping the benefits of FSDP, we utilize Hybrid Sharded Data Parallel (HSDP) to shard the model and optimizer across a set number of GPUs and replicate this multiple times to fully utilize the cluster. With HSDP, an additional all reduce operation is needed in the backward pass to sync gradients across replicas. This approach allows us to balance memory efficiency and communication cost during large scale distributed training. To use HSDP we can extend our previous device mesh from expert parallelism and let PyTorch do the heavy lifting of actually sharding and gathering when needed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg4.png&quot; alt=&quot;그림 4: FSDP와 HSDP&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;그림 4: FSDP와 HSDP / Figure 4: FSDP and HSDP&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;PyTorch를 사용하면 이 두 가지 유형의 병렬화를 효과적으로 결합하여, 전문가 병렬화와 같은 변형(something custom)을 구현할 때 저수준의 &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md&quot;&gt;DTensor&lt;/a&gt; 추상화를 사용하면서도 FSDP의 더 고수준의 API를 활용할 수 있습니다. 이렇게 전문가 병렬화 샤드 차원과 ZeRO-3 샤드 차원, 그리고 순수 데이터 병렬화를 위한 복제 차원으로 구성된 3D 디바이스 메시를 구축할 수 있습니다. 이러한 기법들을 결합하여 매우 큰 클러스터에서 거의 선형적인 확장을 실현할 수 있으며, MFU 수치 40% 이상을 달성할 수 있게 됩니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;With PyTorch, we can effectively combine these two types of parallelism, leveraging FSDP’s higher level API while using the lower-level &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md&quot;&gt;DTensor&lt;/a&gt; abstraction when we want to implement something custom like expert parallelism. We now have a 3D device mesh with expert parallel shard dimension, ZeRO-3 shard dimension, and a replicate dimension for pure data parallelism. Together, these techniques deliver near linear scaling across very large clusters, allowing us to achieve MFU numbers over 40%.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;torch-distributed를-사용한-탄력적인-체크포인팅--elastic-checkpointing-with-torch-distributed&quot;&gt;Torch Distributed를 사용한 탄력적인 체크포인팅 / Elastic Checkpointing with Torch Distributed&lt;/h3&gt;

&lt;p&gt;내결함성(fault tolerance)는 특히 노드 장애가 일반적으로 발생할 수 있는 분산 환경에서 장기간(extended period)에 걸쳐 LLM을 안정적으로 학습시키는데 매우 중요합니다. 작업 중 불가피한 장애가 발생했을 때 진행 상황을 잃지 않기 위해, 매개변수와 옵티마이저 상태, 그리고 다른 필요한 메타데이터를 포함한 모델의 상태를 저장(checkpoint)합니다. 장애가 발생하는 경우, 시스템은 처음부터 다시 시작하지 않고 마지막으로 저장했던 상태에서 다시 시작할 수 있습니다. 장애 시의 견고성(robustness)를 보장하기 위해, 중단 시간(downtime)을 최소화할 수 있는 가장 성능이 좋은 방법으로 체크포인트를 자주 확인하고 저장 및 불러오기를 해야 합니다. 또한, 너무 많은 GPU들에서 장애가 발생하게 되면 클러스터의 크기가 변할 수 있으므로, 다른 수의 GPU에서 탄력적으로 다시 시작할 수 있는 기능이 필요합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Fault tolerance is crucial for ensuring that LLMs can be trained reliably over extended periods, especially in distributed environments where node failures are common. To avoid losing progress when jobs inevitably encounter failures, we checkpoint the state of the model, which includes parameters, optimizer states, and other necessary metadata. When a failure occurs, the system can resume from the last saved state rather than starting over. To ensure robustness to failures, we need to checkpoint often and save and load checkpoints in the most performant way possible to minimize downtime. Additionally, if too many GPUs fail, our cluster size may change. Accordingly, we need the ability to elastically resume on a different number of GPUs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;파이토치(PyTorch)는 다양한 클러스터 구성에서 체크포인트를 저장하고 불러오기 위한 기능(utility)들을 포함하고 있는 분산 학습 프레임워크를 통해 탄력적인 체크포인팅 기능을 지원합니다. 파이토치 분산 체크포인트(PyTorch Distributed Checkpoint)는 노드 장애나 추가로 인한 클러스터 구성이 변경되는 것과 관계없이, 모델의 상태를 학습 클러스터의 모든 노드에서 정확하게 저장하고 복원할 수 있도록 합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;PyTorch supports elastic checkpointing through its distributed training framework, which includes utilities for both saving and loading checkpoints across different cluster configurations. PyTorch Distributed Checkpoint ensures the model’s state can be saved and restored accurately across all nodes in the training cluster in parallel, regardless of any changes in the cluster’s composition due to node failures or additions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또한 매우 큰 모델을 학습할 때, 체크포인트의 크기가 매우 커져서 체크포인트 업로드와 다운로드 시간이 매우 느려질 수 있습니다. 파이토치 분산 체크포인트(PyTorch Distributed Checkpoint)는 분산 체크포인트를 지원하여 각 GPU가 모델의 해당 부분만 저장하고 불러올 수 있도록 합니다. 분산된 체크포인트(sharded checkpoint)를 탄력적 학습(elastic training)과 결합하면 각 GPU는 메타데이터 파일을 읽어 재시작(resumption) 시에 어떠한 부분(shard)을 다운로드할지 결정할 수 있습니다. 메타데이터 파일에는 각 텐서의 어떤 부분이 어떤 샤드(shard)에 저장되어 있는지에 대한 정보가 포함되어 있습니다. 그러면 GPU는 모델의 해당 부분에 대한 샤드를 다운로드하고 체크포인트의 그 부분을 불러올 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Additionally, when training very large models, the size of checkpoints may be very large, leading to very slow checkpoint upload and download times. PyTorch Distributed Checkpoint supports sharded checkpoints, which enables each GPU to save and load only its portion of the model. When combining sharded checkpointing with elastic training, each GPU reads the metadata file to determine which shards to download on resumption. The metadata file contains information on what parts of each tensor are stored in each shard. The GPU can then download the shards for its part of the model and load that part of the checkpoint.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg5.png&quot; alt=&quot;그림 5: 체크포인트 저장하고 추가된 GPU들에서 재개하기&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;그림 5: 체크포인트 저장하고 추가된 GPU들에서 재개하기 / Figure 5: Checkpointing saving and resumption resharded on additional GPUs&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GPU들 간의 체크포인트를 병렬화함으로써 네트워크 부하를 분산시키고 견고성(robustness)과 속도를 향상시킬 수 있습니다. 3000개 이상의 GPU를 사용하여 모델을 학습할 때, 네트워크 대역폭(bandwidth)이 빠르게 병목(bottleneck)이 됩니다. 우리는 먼저 한 복제(replica)에서 체크포인트를 다운로드한 다음, 다른 복제본들에 필요한 부분(shard)들을 보내는 식으로 HSDP의 복제 기능을 활용합니다. &lt;a href=&quot;https://github.com/mosaicml/composer&quot;&gt;Composer&lt;/a&gt;와의 통합을 통해 30분 간격으로 체크포인트를 클라우드 저장소에 안정적으로 업로드하고, 노드 장애 발생 시 자동으로 최신 체크포인트로부터 5분 이내로 재개(resume)할 수 있게 됩니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;By parallelizing checkpointing across GPUs, we can spread out network load, improving robustness and speed. When training a model with 3000+ GPUs, network bandwidth quickly becomes a bottleneck. We take advantage of the replication in HSDP to first download checkpoints on one replica and then send the necessary shards to other replicas. With our integration in &lt;a href=&quot;https://github.com/mosaicml/composer&quot;&gt;Composer&lt;/a&gt;, we can reliably upload checkpoints to cloud storage as frequently as every 30 minutes and automatically resume from the latest checkpoint in the event of a node failure in less than 5 minutes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;결론--conclusion&quot;&gt;결론 / Conclusion&lt;/h2&gt;

&lt;p&gt;파이토치(PyTorch)가 뛰어난 성능으로 최첨단(state-of-the-art) LLM을 학습할 수 있게된 것을 매우 기쁘게 생각합니다. 이번 글에서는 PyTorch Distributed와 MegaBlocks on Foundry를 사용하여 효율적으로 전문가 혼합(MoE) 학습을 구현하는 방법을 보여드렸습니다. 또한, 파이토치(PyTorch) 탄력적 체크포인팅(Elastic Checkpointing)을 사용하여 노드 장애 발생 시 다른 수의 GPU에서 빠르게 학습을 재개할 수 있었습니다. PyTorch HSDP를 사용하여 학습을 효율적으로 확장하고 체크포인트 재개 시간을 개선할 수 있었습니다. 우리는 강력하고 활기찬 오픈소스 커뮤니티를 통해 훌륭한 AI 모델을 모두에게 제공할 수 있기를 기대합니다. &lt;a href=&quot;https://github.com/mosaicml/llm-foundry&quot;&gt;LLM Foundry&lt;/a&gt;와 &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;PyTorch&lt;/a&gt; 저장소를 방문하여 훌륭한 모델을 구축하는데 동참해주세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We’re very excited to see how PyTorch is enabling training state-of-the-art LLMs with great performance. In our post, we’ve shown how we implemented efficient MoE training through PyTorch Distributed and MegaBlocks on Foundry. Furthermore, PyTorch elastic checkpointing allowed us to quickly resume training on a different number of GPUs when node failures occurred. Using PyTorch HSDP has allowed us to scale training efficiently as well as improve checkpointing resumption times. We look forward to continuing building on a strong and vibrant open-source community to help bring great AI models to everyone. Come join us in building great models at &lt;a href=&quot;https://github.com/mosaicml/llm-foundry&quot;&gt;LLM Foundry&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Brian Chu, Mihir Patel, Less Wright, Vitaliy Chiley, Evan Racah, Wanchao Liang, Iris Zhang, Andrew Gu</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">최근 1년간 전문가 혼합(MoE, Mixture-of-Experts) 모델들의 인기가 급증했습니다. 이러한 인기는 DBRX, Mixtral, DeepSeek를 비롯하여 다양하고 강력한 오픈소스 모델들로부터 비롯된 것입니다. Databricks에서는 PyTorch 팀과 협력하여 MoE 모델의 학습을 확장했습니다. 이번 글에서는 PyTorch Distributed 및 PyTorch로 구현한 효율적인 오픈소스 MoE 구현체인 MegaBlocks를 사용하여 학습을 3천개 이상의 GPU들로 확장하는 방법에 대해 이야기해보겠습니다. Over the past year, Mixture of Experts (MoE) models have surged in popularity, fueled by powerful open-source models like DBRX, Mixtral, DeepSeek, and many more. At Databricks, we’ve worked closely with the PyTorch team to scale training of MoE models. In this blog post, we’ll talk about how we scale to over three thousand GPUs using PyTorch Distributed and MegaBlocks, an efficient open-source MoE implementation in PyTorch.</summary></entry><entry><title type="html">PyTorch 2.3 출시 공지</title><link href="https://pytorch.kr/blog/2024/pytorch2-3/" rel="alternate" type="text/html" title="PyTorch 2.3 출시 공지" /><published>2024-04-24T00:00:00+09:00</published><updated>2024-04-24T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2024/pytorch2-3</id><content type="html" xml:base="https://pytorch.kr/blog/2024/pytorch2-3/">&lt;p&gt;PyTorch® 2.3(&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.3.0&quot;&gt;릴리즈 노트&lt;/a&gt;)의 출시를 발표하게 되어 기쁩니다! PyTorch 2.3은 torch.compile()에서 사용자 정의(user-defined) Triton 커널을 지원합니다. 사용자들은 성능 저하나 연산 그래프의 문제 없이 자체 트리톤 커널을 eager 모드에서 torch.compile()로 이전(migration)할 수 있습니다. Tensor Parallelism(텐서 병렬 처리)은 PyTorch 네이티브 함수를 사용하여 대규모 언어 모델(LLM, Large Language Models)을 학습하는 환경을 개선하였으며, 이는 1000억개 규모의 매개변수(100B parameter) 모델 학습을 통해 검증되었습니다. 또한, 반-구조적 희소성(Semi-structured sparsity)은 이를 Tensor 하위클래스(subclass)로 구현되어 밀집 행렬 곱셈(dense matrix multiplication) 대비 최대 1.6배의 속도 향상을 보입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We are excited to announce the release of PyTorch® 2.3 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.3.0&quot;&gt;release note&lt;/a&gt;)! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance regressions or graph breaks. Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models. As well, semi-structured sparsity implements semi-structured sparsity as a Tensor subclass, with observed speedups of up to 1.6 over dense matrix multiplication.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이번 릴리즈는 PyTorch 2.2 이후 426명의 기여자들로부터 3,393회의 커밋으로 구성되었습니다. 헌신적인 커뮤니티의 기여에 진심으로 감사드립니다. 언제나 그렇듯, 새로운 버전을 사용하시며 생기는 문제를 보고해주시면 PyTorch 2.3을 개선하는 데 도움이 됩니다. PyTorch 2 시리즈를 시작하는 방법에 대한 자세한 정보는 &lt;a href=&quot;https://pytorch.kr/get-started/pytorch-2.0/&quot;&gt;시작하기&lt;/a&gt; 페이지에서 확인할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This release is composed of 3393 commits and 426 contributors since PyTorch 2.2. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.3. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;&lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;&lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      torch.compile에서 사용자 정의 Triton 커널 지원&lt;br /&gt;
      &lt;blockquote&gt;User-defined Triton kernels in torch.compile&lt;/blockquote&gt;
   &lt;/td&gt;
   &lt;td&gt;
      torch.export에 dynamic_shape을 지정할 수 있는 새로운 API 추가&lt;br /&gt;
      &lt;blockquote&gt;torch.export adds new API to specify dynamic_shapes&lt;/blockquote&gt;
   &lt;/td&gt;
   &lt;td&gt;
      Inductor CPU 백엔드에 가중치-전용-양자화 도입&lt;br /&gt;
      &lt;blockquote&gt;Weight-Only-Quantization introduced into Inductor CPU backend&lt;/blockquote&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      PyTorch Distributed 내에서 텐서 병렬 처리&lt;br /&gt;
      &lt;blockquote&gt;Tensor parallelism within PyTorch Distributed&lt;/blockquote&gt;
   &lt;/td&gt;
   &lt;td&gt;
      비동기 체크포인트 생성&lt;br /&gt;
      &lt;blockquote&gt;Asynchronous checkpoint generation&lt;/blockquote&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;반-구조적 희소성(Semi-structured sparsity) 지원&lt;br /&gt;
      &lt;blockquote&gt;Support for semi-structured sparsity&lt;/blockquote&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;/td&gt;
   &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;공개된 기능 제출 목록은 &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;여기&lt;/a&gt;에서 확인할 수 있습니다.
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;베타-기능--beta-features&quot;&gt;베타 기능 / Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;베타-사용자-정의-triton-커널에-대한-torchcompile-지원--beta-support-for-user-defined-triton-kernels-in-torchcompile&quot;&gt;[베타] 사용자 정의 Triton 커널에 대한 torch.compile 지원 / [Beta] Support for User-defined Triton kernels in &lt;em&gt;torch.compile&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;torch.compile을 사용하여 Triton 커널이 포함된 PyTorch 코드를 네이티브로 실행할 수 있습니다. 이를 통해 Triton 커널이 포함된 코드를 eager PyTorch에서 &lt;em&gt;torch.compile&lt;/em&gt; 로 성능 저하나 연산 그래프 문제 없이 이전할 수 있습니다. 네이티브 지원은 Torch Inductor가 사용자 정의 Triton 커널을 미리 컴파일(precomile)하여 Triton 커널 주변의 코드를 보다 효율적으로 구성함으로써 추가적인 최적화가 가능하게 합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Allows for PyTorch code that contains triton kernels to be executed natively using torch.compile. This enables users to migrate code containing triton kernels from eager PyTorch to &lt;em&gt;torch.compile&lt;/em&gt; without running into performance regressions or graph breaks. Native support also creates an opportunity for Torch Inductor to precompile the user-defined Triton kernel as well as better organize code around the Triton kernel allowing for further optimizations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;torch.compile에서 사용자 정의 Triton 커널을 활용하는 방법에 대한 자세한 내용은 &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html&quot;&gt;이 튜토리얼&lt;/a&gt;에서 확인하세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You can find more information about how to utilize user defined Triton kernels in torch.compile within &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;베타-텐서-병렬-처리를-통한-llm-학습-효율-향상--beta-tensor-parallelism-introduces-more-efficient-ways-to-train-llms&quot;&gt;[베타] 텐서 병렬 처리를 통한 LLM 학습 효율 향상 / [Beta] Tensor Parallelism introduces more efficient ways to train LLMs&lt;/h3&gt;

&lt;p&gt;텐서 병렬 처리(Tensor Parallel) API는 GPU와 호스트 간의 다양한 텐서 조작을 용이하게 하며, 2D 병렬 처리를 위해 FSDP와 통합되어 있습니다(장치 간 텐서 병렬 처리 + 호스트 간 데이터 병렬 처리). 또한, 고수준(higher-level)의 텐서 병렬 API 구성을 위해 저수준(low-level)의 API들을 제공합니다. 이 API는 1000억 개의 매개변수(100 billion parameters)를 가진 트랜스포머 모델의 학습을 지원함으로써 검증되었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The Tensor Parallel API facilitates various tensor manipulations across GPUs/hosts and integrates with FSDP for 2D Parallelism (Tensor parallelism across devices + Data Parallelism across hosts). It also offers a low-level API for constructing higher-level Tensor parallel APIs. This API has been validated to support the training of transformer models with over 100 billion parameters.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 API를 워크플로우 내에서 활용하는 방법에 대한 자세한 내용은 &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/TP_tutorial.html&quot;&gt;이 튜토리얼&lt;/a&gt;에서 확인하실 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You can find more information on how to utilize this within your workflows within &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/TP_tutorial.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;베타-반-구조적-희소성으로-가속화된-희소-추론-및-메모리-절약--beta-semi-structured-sparsity-provides-users-with-a-way-to-take-advantage-of-accelerated-sparse-inference-and-memory-savings&quot;&gt;[베타] 반-구조적 희소성으로 가속화된 희소 추론 및 메모리 절약 / [Beta] Semi-structured sparsity provides users with a way to take advantage of accelerated sparse inference and memory savings&lt;/h3&gt;

&lt;p&gt;반구조적 희소성을 Tensor의 하위클래스인 &lt;em&gt;torch.sparse.SparseSemiStructuredTensor&lt;/em&gt; 로 구현하였으며 밀집 행렬 곱셈(dense matrix multiplication) 대비 최대 1.6배의 속도 향상을 보였습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;torch.sparse.SparseSemiStructuredTensor&lt;/em&gt; implements semi-structured sparsity as a Tensor subclass, which have observed speedups of up to 1.6 over dense matrix multiplication.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;특히, 다음과 같은 추가적인 기능을 제공합니다:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;양자화 조합 기능(Quantization composability)을 위한 지원(mixed dtype, dequant fusion)&lt;/li&gt;
  &lt;li&gt;업데이트된 cuSPARSELt 및 CUTLASS 커널&lt;/li&gt;
  &lt;li&gt;torch.compile 지원&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;In particular it adds:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Additional support for quantization composability (mixed dtype, dequant fusion)&lt;/li&gt;
    &lt;li&gt;Updated cuSPARSELt and CUTLASS kernels&lt;/li&gt;
    &lt;li&gt;torch.compile support&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;반-구조적 희소성 활용 시 이점에 대한 자세한 내용은 &lt;a href=&quot;https://pytorch.org/tutorials/advanced/semi_structured_sparse.html&quot;&gt;여기&lt;/a&gt;에서 확인하실 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You can find more information on how to take advantage of semi-structured sparsity &lt;a href=&quot;https://pytorch.org/tutorials/advanced/semi_structured_sparse.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;프로토타입-기능--prototype-features&quot;&gt;프로토타입 기능 / Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;프로토타입-torchexport_에-_dynamic_shapes-을-지정하는-api-추가--prototype-torchexport-adds-new-api-to-specify-dynamic_shapes&quot;&gt;[프로토타입] &lt;em&gt;torch.export_에 _dynamic_shapes&lt;/em&gt; 을 지정하는 API 추가 / [PROTOTYPE] &lt;em&gt;torch.export&lt;/em&gt; adds new API to specify &lt;em&gt;dynamic_shapes&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;torch.export.Dim&lt;/em&gt; 을 사용하여 동적 형태(dynamic shapes)를 더 잘 표현할 수 있게 되었습니다. 이를 통해 개발자들은 동일하게 유지되어야 하는 서로 다른 입력 차원들 간의 범위(최소 및 최대 값)를 지정하여 재사용할 수 있게 되었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You can now use &lt;em&gt;torch.export.Dim&lt;/em&gt; to better represent dynamic shapes by enabling developers to specify ranges (min and max values) that can be reused across different input dimensions that are constrained to be equal.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;torch.export.Dim&lt;/em&gt; 에 대한 자세한 내용 및 이를 사용하여 선형 산술 표현식(linear arithmetic expressions)과 같은 더 흥미로운 관계를 표현하는 방법에 대해 더 알아보려면 &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes&quot;&gt;여기&lt;/a&gt;에서 튜토리얼을 확인하실 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To learn more about &lt;em&gt;torch.export.Dim&lt;/em&gt; as well as how it can be used to express more interesting relationships (such as linear arithmetic expressions) check out the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;프로토타입-비동기-체크포인트-생성--prototype-asynchronous-checkpoint-generation&quot;&gt;[프로토타입] 비동기 체크포인트 생성 / [PROTOTYPE] Asynchronous checkpoint generation&lt;/h3&gt;

&lt;p&gt;비동기 체크포인트 생성 기능은 체크포인트가 생성되는 동안 학습 루프를 계속할 수 있도록 하여, 체크포인트 생성 비용의 대부분을 절감(offload)할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Asynchronous checkpoint generation allows users to continue their training loops while checkpoints are being generated, essentially offloading much of the checkpointing cost.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/release/2.3/torch/distributed/checkpoint/examples/async_checkpointing_example.py&quot;&gt;예제&lt;/a&gt;를 통해 이 기능을 워크플로우에서 활용하는 방법을 알아볼 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;You can find out how to utilize this within your own workflows with this &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/release/2.3/torch/distributed/checkpoint/examples/async_checkpointing_example.py&quot;&gt;example&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;성능-개선--performance-improvements&quot;&gt;성능 개선 / Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;프로토타입-inductor-cpu-백엔드에-가중치-전용-양자화-도입--prototype-weight-only-quantization-introduced-into-inductor-cpu-backend&quot;&gt;[프로토타입] Inductor CPU 백엔드에 가중치-전용-양자화 도입 / [PROTOTYPE] Weight-Only-Quantization introduced into Inductor CPU backend&lt;/h3&gt;

&lt;p&gt;PyTorch 2.3에서는 torch inductor CPU 백엔드에서 LLM 추론 성능을 향상시켰습니다. &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;gpt-fast&lt;/a&gt; 프로젝트는 &lt;em&gt;torch.compile&lt;/em&gt; 을 사용하여 트랜스포머 텍스트 생성을 위해 간단하고 효율적인 PyTorch 네이티브 가속 기능을 지원합니다. 2.3 이전에는 CUDA 장치에서만 지원되었던 기능으로, int4 및 int8 가중치 전용 양자화 Linear에 대해 고도로 최적화된 커널을 제공함으로써 CPU 버전을 지원합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;PyTorch 2.3 enhances LLM inference performance on torch inductor CPU backend. The project &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;gpt-fast&lt;/a&gt; offers a simple and efficient PyTorch native acceleration for transformer text generation with &lt;em&gt;torch.compile&lt;/em&gt;. Prior to 2.3 only CUDA devices were supported and this feature enables the CPU counterpart by providing highly optimized kernels for the int4 and int8 weight only quantization Linear.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 기능을 활용하는 방법에 대한 자세한 정보는 &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast#quantization&quot;&gt;gpt-fast README&lt;/a&gt;를 참고해주세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For more information / how to utilize this feature please refer to the &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast#quantization&quot;&gt;gpt-fast README&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>PyTorch Korea User Group</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">PyTorch® 2.3(릴리즈 노트)의 출시를 발표하게 되어 기쁩니다! PyTorch 2.3은 torch.compile()에서 사용자 정의(user-defined) Triton 커널을 지원합니다. 사용자들은 성능 저하나 연산 그래프의 문제 없이 자체 트리톤 커널을 eager 모드에서 torch.compile()로 이전(migration)할 수 있습니다. Tensor Parallelism(텐서 병렬 처리)은 PyTorch 네이티브 함수를 사용하여 대규모 언어 모델(LLM, Large Language Models)을 학습하는 환경을 개선하였으며, 이는 1000억개 규모의 매개변수(100B parameter) 모델 학습을 통해 검증되었습니다. 또한, 반-구조적 희소성(Semi-structured sparsity)은 이를 Tensor 하위클래스(subclass)로 구현되어 밀집 행렬 곱셈(dense matrix multiplication) 대비 최대 1.6배의 속도 향상을 보입니다. We are excited to announce the release of PyTorch® 2.3 (release note)! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance regressions or graph breaks. Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models. As well, semi-structured sparsity implements semi-structured sparsity as a Tensor subclass, with observed speedups of up to 1.6 over dense matrix multiplication.</summary></entry><entry><title type="html">torchtune: PyTorch를 사용한 쉬운 LLM 파인튜닝</title><link href="https://pytorch.kr/blog/2024/torchtune-fine-tune-llms/" rel="alternate" type="text/html" title="torchtune: PyTorch를 사용한 쉬운 LLM 파인튜닝" /><published>2024-04-16T00:00:00+09:00</published><updated>2024-04-16T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2024/torchtune-fine-tune-llms</id><content type="html" xml:base="https://pytorch.kr/blog/2024/torchtune-fine-tune-llms/">&lt;p&gt;대규모 언어 모델(LLM)을 손쉽게 파인튜닝(미세조정)할 수 있는 PyTorch 네이티브 라이브러리인 torchtune의 알파 릴리즈를 발표하게 되어 기쁩니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We’re pleased to announce the alpha release of torchtune, a PyTorch-native library for easily fine-tuning large language models.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;torchtune은 파이토치(PyTorch)의 설계 원칙을 충실히 따랐으며, 다양한 소비자급 및 전문가용 GPU에서 인기있는 LLM들을 파인튜닝할 수 있도록 모듈식 블록 구성과 확장이 쉬운 학습 예시(training recipe)들을 제공합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Staying true to PyTorch’s design principles, torchtune provides composable and modular building blocks along with easy-to-extend training recipes to fine-tune popular LLMs on a variety of consumer-grade and professional GPUs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;torchtune은 다음과 같은 시작부터 끝까지의 파인튜닝 워크플로우를 전반을 지원합니다:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;torchtune supports the full fine-tuning workflow from start to finish, including&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;데이터셋 및 모델 체크포인트 다운로드 및 준비.&lt;/li&gt;
  &lt;li&gt;다양한 모델 아키텍처와 매개변수 효율적 미세 조정(PEFT) 기술 등을 지원하는 빌딩 블록 구성으로 학습 과정의 커스터마이징.&lt;/li&gt;
  &lt;li&gt;학습 과정에서의 진행 상황 및 지표(metric)을 기록하여 인사이트 확보.&lt;/li&gt;
  &lt;li&gt;파인튜닝 후 모델 양자화(quantization).&lt;/li&gt;
  &lt;li&gt;파인튜닝된 모델을 인기 있는 벤치마크들로 평가.&lt;/li&gt;
  &lt;li&gt;파인튜닝된 모델 테스트를 위한 로컬 추론 실행.&lt;/li&gt;
  &lt;li&gt;(파인튜닝한 모델의) 체크포인트와 주로 사용되는 주요 프로덕션 추론 시스템과 호환성.
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;Downloading and preparing datasets and model checkpoints.&lt;/li&gt;
        &lt;li&gt;Customizing the training with composable building blocks that support different model architectures, parameter-efficient fine-tuning (PEFT) techniques, and more.&lt;/li&gt;
        &lt;li&gt;Logging progress and metrics to gain insight into the training process.&lt;/li&gt;
        &lt;li&gt;Quantizing the model post-tuning.&lt;/li&gt;
        &lt;li&gt;Evaluating the fine-tuned model on popular benchmarks.&lt;/li&gt;
        &lt;li&gt;Running local inference for testing fine-tuned models.&lt;/li&gt;
        &lt;li&gt;Checkpoint compatibility with popular production inference systems.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;시작하려면 &lt;a href=&quot;https://www.github.com/pytorch/torchtune&quot;&gt;코드&lt;/a&gt;나다양한 &lt;a href=&quot;https://pytorch.org/torchtune/main/&quot;&gt;튜토리얼&lt;/a&gt;을 살펴보세요!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To get started, jump right into the &lt;a href=&quot;https://www.github.com/pytorch/torchtune&quot;&gt;code&lt;/a&gt; or walk through our many &lt;a href=&quot;https://pytorch.org/torchtune/main/&quot;&gt;tutorials&lt;/a&gt;!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;왜-torchtune을-사용해야-하나요--why-torchtune&quot;&gt;왜 torchtune을 사용해야 하나요? / Why torchtune?&lt;/h2&gt;

&lt;p&gt;지난 한 해 동안 개방형 LLM(Open LLM)에 대한 관심이 폭발적으로 증가했습니다. 최신 모델(SotA, State-of-the-Art)들을 특정한 사용 사례에 맞춰 파인튜닝하는 것이 중요한 기술로 부상했으며, 이러한 적응은 데이터셋 및 모델 선택부터 양자화(Quantization), 평가(Evaluation) 및 추론(Inference)까지 광범위한 사용자 정의를 필요로 하기도 합니다. 게다가 이러한 모델의 크기는 메모리가 제한된 소비자급 GPU에서 파인튜닝을 시도할 때 상당한 어려움을 야기합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Over the past year there has been an explosion of interest in open LLMs. Fine-tuning these state of the art models has emerged as a critical technique for adapting them to specific use cases. This adaptation can require extensive customization from dataset and model selection all the way through to quantization, evaluation and inference. Moreover, the size of these models poses a significant challenge when trying to fine-tune them on consumer-level GPUs with limited memory.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;기존 솔루션들은 사용자 정의(customization)나 최적화(optimization)에 필요한 부분들을 추상화된 계층 뒤에 숨겨놓아, 이러한 기능들을 추가하기 어렵게 만듭니다. 서로 다른 구성 요소가 어떻게 상호 작용하며 새로운 기능을 추가하려면 어떤 부분을 업데이트해야 하는지가 분명하지 않습니다. torchtune은 개발자들에게 완전한 제어와 가시성을 제공하여 특정한 요구 사항과 제약 조건에 맞춰 LLM을 쉽게 적응하고 제어할 수 있도록 지원합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Existing solutions make it hard to add these customizations or optimizations by hiding the necessary pieces behind layers of abstractions. It’s unclear how different components interact with each other and which of these need to be updated to add new functionality. torchtune empowers developers to adapt LLMs to their specific needs and constraints with full control and visibility.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;torchtune의-설계--torchtunes-design&quot;&gt;torchtune의 설계 / torchtune’s Design&lt;/h2&gt;

&lt;p&gt;torchtune은 다음과 같은 원칙을 바탕으로 설계되었습니다:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;torchtune was built with the following principles in mind&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;손쉬운 확장&lt;/strong&gt; - 새로운 기법들이 끊임없이 등장하고 있으며, 모든 파인튜닝 사례는 서로 다릅니다. torchtune의 학습 예시(recipe)는 쉽게 조합(composable)할 수 있는 구성요소들과 변경 가능한(hackable) 학습 루프로 설계되어 있어, 파인튜닝을 어렵게 하는 추상화를 최소화하였습니다. 각 &lt;a href=&quot;https://github.com/pytorch/torchtune/tree/main/recipes&quot;&gt;학습 예시(recipe)들&lt;/a&gt;은 별도의 학습기(trainer)나 프레임워크 없이 독립적으로 구성되어 있으며, 600줄 미만의 코드로 쉽게 읽을 수 있도록 설계되었습니다!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;파인튜닝의 민주화&lt;/strong&gt; - 모든 사용자들이 지식 수준과 상관없이 torchtune을 사용할 수 있도록 하였습니다. 설정(config)을 복제하고 변경하거나 코드를 직접 바꿔보세요! 또한, 데이터센터에 있는 고성능의 GPU가 필요하지 않습니다. 메모리 효율적인 학습 예시들은 24GB 게이밍 GPU를 단지 하나만 장착한 기기에서 테스트되었습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;오픈소스 LLM 생태계와의 상호 운용성&lt;/strong&gt; - 오픈소스 LLM 생태계는 엄청나게 번창하고 있으며, torchtune은 이를 활용하여 다양한 제품군과 상호 운용할 수 있도록 지원합니다. 이러한 유연성을 통해 사용자가 모델을 어떻게 학습하고, 파인튜닝된 모델을 사용할지를 확실하게 제어할 수 있습니다.
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;strong&gt;Easy extensibility&lt;/strong&gt; - New techniques emerge all the time and everyone’s fine-tuning use case is different. torchtune’s recipes are designed around easily composable components and hackable training loops, with minimal abstraction getting in the way of fine-tuning your fine-tuning. Each &lt;a href=&quot;https://github.com/pytorch/torchtune/tree/main/recipes&quot;&gt;recipe&lt;/a&gt; is self-contained - no trainers or frameworks, and is designed to be easy to read - less than 600 lines of code!&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;Democratize fine-tuning&lt;/strong&gt; - Users, regardless of their level of expertise, should be able to use torchtune. Clone and modify configs, or get your hands dirty with some code! You also don’t need beefy data center GPUs. Our memory efficient recipes have been tested on machines with a single 24GB gaming GPU.&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;Interoperability with the OSS LLM ecosystem&lt;/strong&gt; - The open source LLM ecosystem is absolutely thriving, and torchtune takes advantage of this to provide interoperability with a wide range of offerings. This flexibility puts you firmly in control of how you train and use your fine-tuned models.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;내년에는 개방형 LLM(Open LLM)이 더 많은 언어(다국어)와 더 많은 모달리티(멀티모달), 그리고 더 많은 작업들을 지원하며 강력해질 것입니다. 이러한 모델의 복잡성이 증가함에 따라, 우리는 제공되는 기능이나 학습 실행 시의 성능뿐만 아니라 라이브러리를 ‘어떻게’ 설계할지에 대해서도 마찬가지로 주의를 기울여야 합니다. 커뮤니티가 현재의 혁신 속도를 유지하기 위해서는 유연성(flexibility)이 핵심 요소가 될 것이며, 다양한 사용 사례를 지원하기 위해 수많은 라이브러리와 도구들이 서로 잘 동작해야 할 것입니다. torchtune은 처음부터 이러한 미래를 염두에 두고 설계되었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Over the next year, open LLMs will become even more powerful, with support for more languages (multilingual), more modalities (multimodal) and more tasks. As the complexity of these models increases, we need to pay the same attention to “how” we design our libraries as we do to the features provided or performance of a training run. Flexibility will be key to ensuring the community can maintain the current pace of innovation, and many libraries/tools will need to play well with each other to power the full spectrum of use cases. torchtune is built from the ground up with this future in mind.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;진정한 파이토치 정신(True PyTorch Spirit)에 따라, torchtune은 LLM 작업에 주로 사용되는 도구들과의 통합을 제공하여 쉽게 시작할 수 있도록 합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In the true PyTorch spirit, torchtune makes it easy to get started by providing integrations with some of the most popular tools for working with LLMs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://huggingface.co/docs/hub/en/index&quot;&gt;허깅페이스 허브&lt;/a&gt;&lt;/strong&gt; - 허깅페이스는 파인튜닝을 위한 방대한 오픈소스 모델과 데이터셋을 제공합니다. torchtune은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune download&lt;/code&gt; CLI 명령을 통해 쉽게 시작할 수 있도록 허깅페이스 허브를 통합하여 파인튜닝을 바로 시작할 수 있도록 지원합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;PyTorch FSDP&lt;/a&gt;&lt;/strong&gt; - PyTorch FSDP를 사용하여 학습을 확장할 수 있습니다. 일반적으로 NVIDIA의 3090/4090과 같은 소비자급 GPU를 장착한 기기들을 많이 사용하고 있습니다. torchtune은 FSDP 기반의 분산 학습 예시를 제공하여 이러한 설정을 활용할 수 있도록 지원합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://wandb.ai/site&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt;&lt;/strong&gt; - torchtune은 Weights &amp;amp; Biases의 AI 플랫폼을 사용하여 학습 중 지표(metric)들과 모델의 체크포인트를 기록합니다. 파인튜닝 실행 중, 설정과 메트릭 및 모델 등을 한 곳에서 한꺼번에 추적할 수 있습니다!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;EleutherAI의 언어모델 평가도구&lt;/a&gt;&lt;/strong&gt; - 파인튜닝을 통해 원하는 결과를 얻을 수 있는지 여부를 이해하기 위해서는 파인튜닝된 모델의 평가(Evaluation)가 중요합니다. torchtune은 EleutherAI의 언어모델(LM) 평가도구(Evaluation Harness)를 활용하여 일반적으로 많이 사용하는 LLM 벤치마크 모음에 쉽게 접근할 수 있는 간단한 평가 학습 예시를 제공합니다. 평가 과정의 중요성을 고려하여, 앞으로 몇 달 동안 EleutherAI와 밀접하게 협력하여 더 깊고 더 “네이티브(native)”하게 통합할 예정입니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/executorch-overview&quot;&gt;ExecuTorch&lt;/a&gt;&lt;/strong&gt; - torchtune으로 파인튜닝된 모델은 ExecuTorch로 &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning&quot;&gt;쉽게 내보내기(export)&lt;/a&gt;할 수 있어, 다양한 모바일 및 엣지 장치(Edge Device)에서 효율적인 추론을 실행할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-labs/ao&quot;&gt;torchao&lt;/a&gt;&lt;/strong&gt; - torchao의 양자화 API(Quantization API)를 활용한 간단한 &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/quantize.py&quot;&gt;학습 후 양자화 예시(Post-training recipe)&lt;/a&gt;를 통해 파인튜닝된 모델을 4비트 또는 8비트로 쉽게 양자화할 수 있습니다.
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://huggingface.co/docs/hub/en/index&quot;&gt;Hugging Face Hub&lt;/a&gt;&lt;/strong&gt; - Hugging Face provides an expansive repository of open source models and datasets for fine-tuning. torchtune seamlessly integrates through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune download&lt;/code&gt; CLI command so you can get started right away with fine-tuning your first model.&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;PyTorch FSDP&lt;/a&gt;&lt;/strong&gt; - Scale your training using PyTorch FSDP. It is very common for people to invest in machines with multiple consumer level cards like the 3090/4090 by NVidia. torchtune allows you to take advantage of these setups by providing distributed recipes powered by FSDP.&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://wandb.ai/site&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt;&lt;/strong&gt; - torchtune uses the Weights &amp;amp; Biases AI platform to log metrics and model checkpoints during training. Track your configs, metrics and models from your fine-tuning runs all in one place!&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;EleutherAI’s LM Evaluation Harness&lt;/a&gt;&lt;/strong&gt; - Evaluating fine-tuned models is critical to understanding whether fine-tuning is giving you the results you need. torchtune includes a simple evaluation recipe powered by EleutherAI’s LM Evaluation Harness to provide easy access to a comprehensive suite of standard LLM benchmarks. Given the importance of evaluation, we will be working with EleutherAI very closely in the next few months to build an even deeper and more “native” integration.&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/executorch-overview&quot;&gt;ExecuTorch&lt;/a&gt;&lt;/strong&gt; - Models fine-tuned with torchtune can be &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning&quot;&gt;easily exported&lt;/a&gt; to ExecuTorch, enabling efficient inference to be run on a wide variety of mobile and edge devices.&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-labs/ao&quot;&gt;torchao&lt;/a&gt;&lt;/strong&gt; - Easily and efficiently quantize your fine-tuned models into 4-bit or 8-bit using a simple &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/quantize.py&quot;&gt;post-training recipe&lt;/a&gt; powered by the quantization APIs from torchao.&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;다음-단게는-무엇인가요--whats-next&quot;&gt;다음 단게는 무엇인가요? / What’s Next?&lt;/h2&gt;

&lt;p&gt;이것은 시작에 불과하며 활기차고 에너지가 넘치는 커뮤니티에 이번 알파 버전을 제공하게 되어 매우 기쁩니다. 앞으로 몇 주 동안, 더 많은 모델과 기능 및 파인튜닝 기법들을 추가할 예정입니다. 피드백이나 의견, 기능 요청은 GitHub 저장소의 이슈(issue)나 &lt;a href=&quot;https://discord.com/invite/4Xsdn8Rr9Q&quot;&gt;Discord 채널&lt;/a&gt;로 보내주시기 바랍니다. 언제나 그렇듯, 이 멋진 커뮤니티로부터의 모든 기여를 환영합니다. 즐거운 파인튜닝하세요!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This is just the beginning and we’re really excited to put this alpha version in front of a vibrant and energetic community. In the coming weeks, we’ll continue to augment the library with more models, features and fine-tuning techniques. We’d love to hear any feedback, comments or feature requests in the form of GitHub issues on our repository, or on our &lt;a href=&quot;https://discord.com/invite/4Xsdn8Rr9Q&quot;&gt;Discord channel&lt;/a&gt;. As always, we’d love any contributions from this awesome community. Happy Tuning!&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>PyTorch Korea User Group</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">대규모 언어 모델(LLM)을 손쉽게 파인튜닝(미세조정)할 수 있는 PyTorch 네이티브 라이브러리인 torchtune의 알파 릴리즈를 발표하게 되어 기쁩니다. We’re pleased to announce the alpha release of torchtune, a PyTorch-native library for easily fine-tuning large language models.</summary></entry><entry><title type="html">PyTorch 2 논문 및 튜토리얼 @ ASPLOS 2024</title><link href="https://pytorch.kr/blog/2024/pytorch-2-paper-tutorial/" rel="alternate" type="text/html" title="PyTorch 2 논문 및 튜토리얼 @ ASPLOS 2024" /><published>2024-02-06T00:00:00+09:00</published><updated>2024-02-06T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2024/pytorch-2-paper-tutorial</id><content type="html" xml:base="https://pytorch.kr/blog/2024/pytorch-2-paper-tutorial/">&lt;p&gt;2024년 4월 27일부터 5월 1일까지 미국 캘리포니아주 샌디에이고에서 열릴 예정인 ACM 국제 컨퍼런스 ASPLOS(Architectural Support for Programming Languages and Operating Systems)에서 PyTorch 2에 대한 논문이 선정되어 발표하게 되었다는 소식을 전할 수 있어 기쁘게 생각합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The PyTorch team is excited to share that our paper on PyTorch 2 has been accepted for presentation at the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), scheduled to take place from April 27 to May 1, 2024, in San Diego, CA, USA.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 논문에서는 torch.compile의 구현에 대해 자세히 살펴보며, 특히 이를 위한 주요 기술들인 TorchDynamo(그래프 캡처), TorchInductor(백엔드 컴파일러) 및 Dynamic Shape 지원 등을 중점적으로 다루고 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The paper delves into the implementation of torch.compile and highlights the key technologies driving it, including TorchDynamo (graph capture), TorchInductor (backend compiler), and Dynamic Shape support.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ASPLOS 컨퍼런스 기간 중인 4월 27일(토)에 시스템 연구자들을 위해 PyTorch 2의 내부 동작 방식과 이를 활용하고 구축할 수 있는 방법에 초점을 맞춘 튜토리얼을 진행할 예정입니다. 행사 일정에 맞춰 세부적인 내용이 확정되는대로 공유드리도록 하겠습니다. 많은 참여 기대합니다!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;During the ASPLOS conference, we’ll be conducting a tutorial on Saturday, April 27, focusing on the inner workings of PyTorch 2 and how systems researchers can leverage and build upon it. Stay tuned for more details as the event approaches – we look forward to your participation!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;논문의 미리보기는 아래 첨부하였습니다:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A preview of the paper is attached below:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;제목:  &lt;strong&gt;PyTorch 2: 동적 Python 바이트코드 변환과 그래프 컴파일을 통한 더 빠른 머신 러닝&lt;/strong&gt; &lt;a href=&quot;/assets/pytorch2-2.pdf&quot;&gt;&lt;strong&gt;논문 전문 PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Title: &lt;strong&gt;PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.&lt;/strong&gt; &lt;a href=&quot;/assets/pytorch2-2.pdf&quot;&gt;&lt;strong&gt;Full Paper PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;논문-초록-abstract&quot;&gt;논문 초록 (Abstract)&lt;/h3&gt;

&lt;p&gt;이 논문에서는 인기있는 파이토치(PyTorch) 머신러닝 프레임워크의 두 가지 확장 기능인 TorchDynamo와 TorchInductor를 소개합니다. 이 두 확장 기능은 PyTorch 2에서 발표된 torch.compile 기능을 구현하기 위한 것입니다. TorchDynamo는 Python 수준의 JIT(Just-in-Time) 컴파일러로, Python의 유연성을 희생하지 않으면서 PyTorch 프로그램에서 그래프 컴파일(graph compilation)을 가능하게 합니다. 이를 위해 TorchDynamo는 Python 바이트코드(bytecode)를 실행 전 동적으로 수정하고 PyTorch 연산 시퀀스를 FX 그래프로 추출한 다음, 확장 가능한 다양한 백엔드 중 하나를 사용하여 JIT 컴파일을 수행합니다. TorchInductor는 TorchDynamo의 기본 컴파일 백엔드로, PyTorch 프로그램을 OpenAI의 Triton 및 CPU용 C++ 코드로 변환(translate)합니다. 실험 결과, TorchDynamo는 최소한의 오버헤드만으로 이전의 접근 방식보다 더 견고(robust)하게 그래프를 캡쳐할 수 있으며, TorchInductor는 MVIDIA A100 GPU에서 180개 이상의 실제 사용 모델(180+ real-world models)에 대해서 학습 시 1.14배와 추론 시 2.27배의 평균적 속도 향상(기하 평균, geometric mean)을 보이는 것으로 나타났습니다. 이러한 확장 기능들은 PyTorch와 같은 Eager 방식의 프레임워크에서 컴파일러를 통해 최적화를 적용하는 새로운 방법을 제공합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI’s Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27x inference and 1.41x training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;논문-저자-authors&quot;&gt;논문 저자 (Authors)&lt;/h3&gt;

&lt;p&gt;Jason Ansel (Meta); Edward Yang (Meta); Horace He (Meta); Natalia Gimelshein (OpenAI); Animesh Jain (Meta); Michael Voznesensky (Meta); Bin Bao (Meta); Peter Bell (Quansight); David Berard (Meta); Evgeni Burovski Quansight; Geeta Chauhan (Meta); Anjali Chourdia (Meta); Will Constable (Meta); Alban Desmaison (Meta); Zachary DeVito (Meta); Elias Ellison (Meta); Will Feng (Meta); Jiong Gong (Intel); Michael Gschwind (Meta); Brian Hirsh (Meta); Sherlock Huang (Meta); Kshiteej Kalambarkar (Quansight); Laurent Kirsch (Meta); Michael Lazos (Meta); Mario Lezcano (Quansight); Yanbo Liang (Meta); Jason Liang (Meta); Yinghai Lu (Meta); CK Luk (Meta); Bert Maher (Meta); Yunjie Pan (University of Michigan); Christian Puhrsch (Meta); Matthias Reso (Meta); Mark Saroufim (Meta); Marcos Yukio Siraichi (Quansight); Helen Suk (Meta); Michael Suo (Meta); Phil Tillet (OpenAI); Eikan Wang (Intel); Xiaodong Wang (Meta); William Wen (Meta); Shunting Zhang (Meta); Xu Zhao (Meta); Keren Zhou (OpenAI &amp;amp; George Mason University); Richard Zou (Meta); Ajit Mathews (Meta); Gregory Chanan (Meta); Peng Wu (Meta); Soumith Chintala (Meta)&lt;/p&gt;

&lt;h3 id=&quot;asplos24---full-day-tutorial-schedule&quot;&gt;ASPLOS’24 - Full Day Tutorial Schedule&lt;/h3&gt;

&lt;p&gt;ASPLOS’24의 파이토치 2(PyTorch 2) 튜토리얼은 4월 27일, 토요일에 진행됩니다. 자세한 일정은 &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/ASPLOS_2024&quot;&gt;여기&lt;/a&gt;를 참고해 주세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Full schedule for the ASPLOS’24 PyTorch 2 Tutoral on Saturday, April 27th is available &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/ASPLOS_2024&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>PyTorch Korea User Group</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">2024년 4월 27일부터 5월 1일까지 미국 캘리포니아주 샌디에이고에서 열릴 예정인 ACM 국제 컨퍼런스 ASPLOS(Architectural Support for Programming Languages and Operating Systems)에서 PyTorch 2에 대한 논문이 선정되어 발표하게 되었다는 소식을 전할 수 있어 기쁘게 생각합니다. The PyTorch team is excited to share that our paper on PyTorch 2 has been accepted for presentation at the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), scheduled to take place from April 27 to May 1, 2024, in San Diego, CA, USA.</summary></entry><entry><title type="html">PyTorch 2.1에 새로 추가된 성능 향상 기능 소개</title><link href="https://pytorch.kr/blog/2023/new-features-for-ai/" rel="alternate" type="text/html" title="PyTorch 2.1에 새로 추가된 성능 향상 기능 소개" /><published>2023-11-29T00:00:00+09:00</published><updated>2023-11-29T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2023/new-features-for-ai</id><content type="html" xml:base="https://pytorch.kr/blog/2023/new-features-for-ai/">&lt;p&gt;PyTorch(파이토치) 2.1을 출시하여 매우 기쁩니다. 이번 글에서는 PyTorch 2.1에 인텔(Intel)이 크게 기여한 다섯가지 기능들에 대해서 설명하겠습니다:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We are excited to see the release of PyTorch 2.1. In this blog, we discuss the five features for which Intel made significant contributions to PyTorch 2.1:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;torch.compile()시 bfloat16 추론 경로를 포함하는 TorchInductor-CPU 최적화&lt;/li&gt;
  &lt;li&gt;torch.compile()에서 CPU용 동적 쉐입(dynamic shape) 추론 경로&lt;/li&gt;
  &lt;li&gt;C++ 래퍼 (wrapper, 프로토타입 기능)&lt;/li&gt;
  &lt;li&gt;CPU용 플래시 어텐션(flash-attention) 기반 스케일드-닷-프로덕트(scaled-dot-product) 알고리즘&lt;/li&gt;
  &lt;li&gt;Inductor를 통해 x86 백엔드를 사용한 PyTorch 2의 학습 후 양자화 내보내기 기능
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;TorchInductor-CPU optimizations including Bfloat16 inference path for torch.compile&lt;/li&gt;
        &lt;li&gt;CPU dynamic shape inference path for torch.compile&lt;/li&gt;
        &lt;li&gt;C++ wrapper (prototype)&lt;/li&gt;
        &lt;li&gt;Flash-attention-based scaled dot product algorithm for CPU&lt;/li&gt;
        &lt;li&gt;PyTorch 2 export post-training qauantization with an x86 back end through an inductor&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;인텔이 PyTorch 커뮤니티의 일원이 된 것을 기쁘게 생각하며, Meta*의 동료들과 이러한 기능들을 함께 개발하고 피드백을 주신 것에 감사드립니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;At Intel, we are delighted to be part of the PyTorch community and appreciate the collaboration with and feedback from our colleagues at Meta* as we co-developed these features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이제 시작해보겠습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Let’s get started.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;torchinductor-cpu-최적화--torchinductor-cpu-optimizations&quot;&gt;TorchInductor-CPU 최적화 / TorchInductor-CPU Optimizations&lt;/h2&gt;

&lt;p&gt;이 기능은 TorchInductor(토치인덕터)의 bfloat16 추론 성능을 최적화합니다. 3세대 및 4세대의 Intel® Xeon® Scalable 프로세서에는 bfloat16 데이터 타입의 닷-프로덕트(dot-product) 연산을 가속화하는 하드웨어 가속기가 내장되어 있습니다. 아래 그림 1은 BF16 추론 경로를 지정하는 코드 조각입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This feature optimizes bfloat16 inference performance for TorchInductor. The 3rd and 4th generation Intel® Xeon® Scalable processors have built-in hardware accelerators for speeding up dot-product computation with the bfloat16 data type. Figure 1 shows a code snippet of how to specify the BF16 inference path.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;user_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;user_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autocast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그림 1. TorchInductor에서 BF16 추론 사용 예시 코드 / Figure 1. Code snippet showing the use of BF16 inference with TorchInductor &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;TorchBench, Hugging Face&lt;em&gt;, TIMM의 3종을 벤치마크 대상으로하여 TorchInductor의 성능을 측정했으며 그 결과는 표 1과 같습니다. 그래프 모드(TorchInductor)의 성능이 Eager 모드보다 1.25배에서 2.35배&lt;/em&gt;까지 뛰어난 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We measured the performance on three TorchInductor benchmark suites—TorchBench, Hugging Face&lt;em&gt;, and TIMM—and the results are as follows in Table 1. Here we see that performance in graph mode (TorchInductor) outperforms eager mode by factors ranging from 1.25x to 2.35x.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;표 1. 그래프 모드와 Eager 모드에서의 bfloat16 성능 향상 (기하 평균, geometric mean) / Table 1. Bfloat16 performance geometric mean speedup in graph mode, compared with eager mode&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Bfloat16 Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.81x
   &lt;/td&gt;
   &lt;td&gt;
1.25x
   &lt;/td&gt;
   &lt;td&gt;
2.35x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Bfloat16 Geometric Mean Speedup (Single-Core Single Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.74x
   &lt;/td&gt;
   &lt;td&gt;
1.28x
   &lt;/td&gt;
   &lt;td&gt;
1.29x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Intel® Advanced Matrix Extensions(Intel® AMX) 기능을 활용하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; 성능을 최대치로 끌어냄으로써 4세대 인텔 Xeon 프로세서에서 모델을 완전히 배포할 수 있습니다. 인텔 AMX에는 타일(Tile)과 타일드 매트릭스 곱셈(TMUL; Tiled Matric Multiplication)이라는 두 가지 주요 구성 요소가 있습니다. 타일(tile)은 각각 1KB 크기를 갖는 2차원 레지스터 8개에 대량의 데이터를 저장합니다. TMUL은 한 번의 연산(single operation)으로 더 큰 행렬을 계산하기 위한 명령어들을 지원하는, 타일에 연결된 가속 엔진입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Developers can fully deploy their models on 4th generation Intel Xeon processors to take advantage of the Intel® Advanced Matrix Extensions (Intel® AMX) feature to get peak performance for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. Intel AMX has two primary components: tiles and tiled matrix multiplication (TMUL). The tiles store large amounts of data in eight two-dimensional registers, each one kilobyte in size. TMUL is an accelerator engine attached to the tiles that contain instructions to compute larger matrices in a single operation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;torchcompile에서-cpu용-동적-쉐입dynamic-shape-추론-경로--cpu-dynamic-shapes-inference-path-for-torchcompile&quot;&gt;torch.compile()에서 CPU용 동적 쉐입(dynamic shape) 추론 경로 / CPU Dynamic Shapes Inference Path for torch.compile&lt;/h2&gt;

&lt;p&gt;동적 쉐입(Dynamic Shape)은 PyTorch 2.0의 핵심 기능들 중 하나입니다. PyTorch 2.0은 기본적으로 모든 것이 정적이라고 가정하고 있습니다. 크기가 변경되어 다시 컴파일을 해야 할 때는 동적 크기를 갖도록 다시 컴파일하려고  시도합니다. (한 번 변경된 크기는 앞으로 변경될 가능성이 높습니다.) 동적 쉐입을 지원하는 경우, conv/gemm 연산자에 대한 후처리 퓨전(post-op fusion)과 conv/gemm 연산자가 아닌 연산자(non-conv-gemm operators)에 대한 벡터화 코드 생성(vectorization code-gen)을 지원합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Dynamic shapes is one of the key features in PyTorch 2.0. PyTorch 2.0 assumes everything is static by default. If we recompile because a size changed, we will instead attempt to recompile that size as being dynamic (sizes that have changed are likely to change in the future). Dynamic shapes support is required for popular models like large language models (LLM). Dynamic shapes that provide support for a broad scope of models can help users get more benefit from torch.compile. For dynamic shapes, we provide the post-op fusion for conv/gemm operators and vectorization code-gen for non-conv/gemm operators.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;동적 쉐입은 CUDA*용 인덕터 트리튼 백엔드와 CPU용 C++ 백엔드에서 모두 지원하고 있습니다. 여기에는 기능(모델 통과율로 측정)과 성능(추론 지연/처리량으로 측정) 모두에 대한 개선이 포함하고 있습니다. 그림 2는 TorchInductor에서 동적 쉐입 추론을 사용하는 코드 조각을 보여줍니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Dynamic shapes is supported by both the inductor Triton back end for CUDA* and the C++ back end for CPU. The scope covers improvements for both functionality (as measured by model passing rate) and performance (as measured by inference latency/throughput). Figure 2 shows a code snippet for the use of dynamic shape inference with TorchInductor.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;user_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 학습 예시 / Training example
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_size1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 여기서 입력 크기가 변경되어 재컴파일을 시작합니다
# Here trigger the recompile because the input size changed
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# 추론 예시 / Inference example
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_size1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;# 여기서 입력 크기가 변경되어 재컴파일을 시작합니다
&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Here trigger the recompile because the input size changed
&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그림 2. TorchInductor로 동적 쉐입 추론을 사용하는 코드 조각 / Figure 2. Code snippet showing the use of dynamic shape inference with TorchInductor&lt;/p&gt;

&lt;p&gt;TorchInductor 벤치마크 제품군 3종(TorchBench, Hugging Face, TIMM)에 대해서 다시 성능을 측정한 결과를 표 2에 정리하였습니다. 여기서 그래프 모드의 성능이 Eager 모드보다 1.15배에서 1.79배까지 뛰어난 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We again measured the performance on the three TorchInductor benchmark suites—TorchBench, Hugging Face, and TIMM—and the results are in Table 2. Here we see that performance in graph mode outperforms eager mode by factors ranging from 1.15x to 1.79x.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;표 2. 동적 쉐입에서 Eager 모드 대비 성능 향상 (기하 평균, geometric mean) / Table 2. Dynamic shape geometric mean speedup compared with Eager mode&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Dynamic Shape Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.35x
   &lt;/td&gt;
   &lt;td&gt;
1.15x
   &lt;/td&gt;
   &lt;td&gt;
1.79x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;Dynamic Shape Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
torchbench
   &lt;/td&gt;
   &lt;td&gt;
huggingface
   &lt;/td&gt;
   &lt;td&gt;
timm_models
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.48x
   &lt;/td&gt;
   &lt;td&gt;
1.15x
   &lt;/td&gt;
   &lt;td&gt;
1.48x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;c-래퍼프로토타입-기능--c-wrapper-prototype&quot;&gt;C++ 래퍼(프로토타입 기능) / C++ Wrapper (Prototype)&lt;/h2&gt;

&lt;p&gt;이 기능은 TorchInductor의 생성된 커널 및 외부 커널을 호출하는 Python* 코드 대신 C++ 코드를 생성하여 Python 오버헤드를 줄입니다. 또한 Python이 없는 환경에서의 배포를 지원하기 위한 중간 단계이기도 합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The feature generates C++ code instead of Python* code to invoke the generated kernels and external kernels in TorchInductor to reduce Python overhead. It is also an intermediate step to support deployment in environments without Python.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;다음과 같이 설정하면 이 기능을 사용할 수 있습니다:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To enable this feature, use the following configuration:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch._inductor.config&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpp_wrapper&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Python 래퍼에서의 오버헤드가 더 큰, 가벼운 워크로드의 경우에는 C++ 래퍼가 더 높은 성능 향상 비율을 보여줍니다. TorchBench, Hugging Face, TIMM의 모델들을 평균 추론 시간에 따라 Small, Medium, Large의 세 가지 그룹으로 정리하였습니다. 표 3은 C++ 래퍼가 기본 Python 래퍼 대비 얻은 기하 평균 성능 향상 비율을 보여줍니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For light workloads where the overhead of the Python wrapper is more dominant, C++ wrapper demonstrates a higher performance boost ratio. We grouped the models in TorchBench, Hugging Face, and TIMM per the average inference time of one iteration and categorized them into small, medium, and large categories. Table 3 shows the geometric mean speedups achieved by the C++ wrapper in comparison to the default Python wrapper.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;표 3. Eager 모드와 비교한 C++ 래퍼의 속도 향상 (기하 평균, geometric mean) / Table 3. C++ wrapper geometric mean speedup compared with Eager mode&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Static Shape Mode Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.06x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
   &lt;td&gt;
1.00x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Static Shape Mode Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.13x
   &lt;/td&gt;
   &lt;td&gt;
1.02x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Dynamic Shape Mode Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.05x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
   &lt;td&gt;
1.00x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;FP32 Dynamic Shape Mode Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.14x
   &lt;/td&gt;
   &lt;td&gt;
1.02x
   &lt;/td&gt;
   &lt;td&gt;
1.01x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;BF16 Static Shape Mode Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.09x
   &lt;/td&gt;
   &lt;td&gt;
1.03x
   &lt;/td&gt;
   &lt;td&gt;
1.04x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;4&quot;&gt;
&lt;strong&gt;BF16 Static Shape Mode Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Small (t &amp;lt;= 0.04s)
   &lt;/td&gt;
   &lt;td&gt;
Medium (0.04s &amp;lt; t &amp;lt;= 1.5s)
   &lt;/td&gt;
   &lt;td&gt;
Large (t &amp;gt; 1.5s)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.17x
   &lt;/td&gt;
   &lt;td&gt;
1.04x
   &lt;/td&gt;
   &lt;td&gt;
1.03x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;cpu용-플래시-어텐션flash-attention-기반-스케일드-닷-프로덕트scaled-dot-product-알고리즘--flash-attention-based-scaled-dot-product-algorithm-for-cpu&quot;&gt;CPU용 플래시 어텐션(flash-attention) 기반 스케일드-닷-프로덕트(scaled-dot-product) 알고리즘 / Flash-Attention-Based Scaled Dot Product Algorithm for CPU&lt;/h2&gt;

&lt;p&gt;스케일-닷-프로덕트 어텐션(SDPA)은 Transformer 모델들의 속도 향상을 지원하는 PyTorch 2.0의 핵심 기능 중 하나입니다. 이 기능은 최적의 CUDA 커널을 사용하여 가속하지만, 최적화된 CPU의 커널은 아직 없었습니다. 이 플래시-어텐션(flash-attention) 구현은 FP32 및 bfloat16 데이터 타입에서의 학습과 추론 모두를 지원합니다. 사용자는 이 SDPA 최적화를 활용하기 위해 코드(frontend)를 변경할 필요는 없습니다. SDPA 호출 시에 자동적으로 구현체를 선택하는데, 이 새로운 구현체를 포함하고 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Scaled dot product attention (SDPA) is one of the flagship features of PyTorch 2.0 that helps speed up transformer models. It is accelerated with optimal CUDA kernels while still lacking optimized CPU kernels. This flash-attention implementation targets both training and inference, with both FP32 and Bfloat16 data types supported. There is no front-end use change for users to leverage this SDPA optimization. When calling SDPA, a specific implementation will be chosen automatically, including this new implementation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;허깅페이스의 SDPA 관련 모델들을 측정한 결과, 이 기능이 없는 SDPA(unfused SDPA)보다 효과적인 것을 확인하였습니다. 표 4에는 SDPA 최적화에 대한 기하 평균 속도 향상 비율을 보여줍니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We have measured the SDPA-related models in Hugging Face, and they are proven effective when compared to the unfused SDPA. Shown in Table 4 are the geometric mean speedups for SDPA optimization. &lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;표 4. SDPA 최적화 속도 향상 (기하 평균, geometric mean) / Table 4. SDPA optimization performance geometric mean speedup&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;3&quot;&gt;
&lt;strong&gt;SDPA Geometric Mean Speedup (Single-Socket Multithreads)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup FP32
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup BF16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.15x, 20/20
   &lt;/td&gt;
   &lt;td&gt;
1.07x, 20/20
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;3&quot;&gt;
&lt;strong&gt;SDPA Geometric Mean Speedup (Single-Core Single-Thread)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
Compiler
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup FP32
   &lt;/td&gt;
   &lt;td&gt;
Geometric Speedup BF16
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
1.02x, 20/20
   &lt;/td&gt;
   &lt;td&gt;
1.04x, 20/20
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;inductor를-통해-x86-백엔드를-사용한-학습-후-양자화-pytorch-2-내보내기-pytorch-2-export-post-training-quantization-with-x86-back-end-through-inductor&quot;&gt;Inductor를 통해 x86 백엔드를 사용한 학습 후 양자화 PyTorch 2 내보내기/ PyTorch 2 Export Post-Training Quantization with x86 Back End through Inductor&lt;/h2&gt;

&lt;p&gt;PyTorch는 PyTorch 2.0 내보내기(export)에서 새로운 양자화 흐름(quantization flow)을 제공하고 있습니다. 이 기능은 x86 CPU 장치를 백엔드로 하는 TorchInductor를 사용하여, 새로운 양자화 흐름을 적용한 학습 후 정적 양자화(post-training quantization)를 수행합니다. 예시 코드 조각은 그림 3과 같습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;PyTorch provides a new quantization flow in the PyTorch 2.0 export. This feature uses TorchInductor with an x86 CPU device as the back end for post-training static quantization with this new quantization flow. An example code snippet is shown in Figure 3.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch._dynamo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchdynamo&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.ao.quantization.quantize_pt2e&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_pt2e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prepare_pt2e&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.ao.quantization.quantizer.x86_inductor_quantizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xiq&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;# 1단계: 모델을 평탄화(flatten)한 ATen 연산자의 FX 그래프로 추적합니다
&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Step 1: Trace the model into an FX graph of flattened ATen operators
&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exported_graph_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guards&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchdynamo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;export&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
	 &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	 &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deepcopy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
	 &lt;span class=&quot;n&quot;&gt;aten_graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;c1&quot;&gt;# 2단계: 관찰자(observers) 또는 가짜 양자화 모듈을 삽입합니다
&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Step 2: Insert observers or fake quantize modules
&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xiq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X86InductorQuantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;operator_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xiq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_x86_inductor_quantization_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;operator_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;prepared_graph_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prepare_pt2e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exported_graph_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quantizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;c1&quot;&gt;# 여기서 캘리브레이션을 수행합니다
&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Doing calibration here.
&lt;/span&gt;
 &lt;span class=&quot;c1&quot;&gt;# 3단계: 모델을 양자화합니다
&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Step 3: Quantize the model
&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_graph_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_pt2e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepared_graph_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;c1&quot;&gt;# 4단계: 백엔드로 양자화된 모델을 내보냅니다
&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Step 4: Lower Quantized Model into the backend
&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compile_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_graph_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그림 3. Inductor를 PyTorch 2의 학습 후 양자화 내보내기를 위한 백엔드로 사용하는 코드 조각 / Figure 3. Code snippet showing the use of Inductor as back end for PyTorch 2 export post-training quantization&lt;/p&gt;

&lt;p&gt;모든 합성곱 신경망(CNN; Convolutional Neural Network) 모델들은 TorchBench 벤치마크 테스트 스윗(suite)에서 측정하였으며, Inductor FP32 추론 경로와 비교하여 효과적임이 증명되었습니다. 성능 지표는 표 5와 같습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;All convolutional neural networks (CNN) models from the TorchBench test suite have been measured and proven effective when compared with the Inductor FP32 inference path. Performance metrics are shown in Table 5.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;표 5. Inductor를 통해 x86 백엔드를 사용한 학습 후 양자화 내보내기 성능 향상 (기하 평균, geometric mean) / Table 5. PyTorch 2 export post-training quantization performance geometric mean speedup with x86 back end through Inductor&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Compiler&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Geometric Speedup&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Geometric Related Accuracy Loss&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
inductor
   &lt;/td&gt;
   &lt;td&gt;
3.25x, 12/12
   &lt;/td&gt;
   &lt;td&gt;
0.44%, 12/12
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;다음-단계--next-steps&quot;&gt;다음 단계 / Next Steps&lt;/h2&gt;

&lt;h3 id=&quot;소프트웨어-다운로드--get-the-software&quot;&gt;소프트웨어 다운로드 / Get the Software&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.1.0&quot;&gt;PyTorch 2.1&lt;/a&gt;을 사용해보고 인텔이 기여한 이러한 기능들의 성능 이점을 직접 확인해보세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Try out &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.1.0&quot;&gt;PyTorch 2.1&lt;/a&gt; and realize the performance benefits for yourself from these features contributed by Intel.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;인텔의 다른 &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI 도구들&lt;/a&gt;과 &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;프레임워크&lt;/a&gt; 최적화를 확인하고 인텔의 AI 소프트웨어 포트폴리오의 기반이 되는 오픈, 표준 기반 &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; 멀티아키텍처, 멀티벤더 프로그래밍 모델에 대해 알아보세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We encourage you to check out Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;framework&lt;/a&gt; optimizations and learn about the open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;4세대 인텔 Xeon Scalable 프로세서에 대한 자세한 내용은 &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI 플랫폼&lt;/a&gt;에서 확인할 수 있으며, 개발자들이 고성능의 효율적인 엔드-투-엔드 AI 파이프라인을 실행할 수 있도록 지원하고 있는 방법에 대해 알아보세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For more details about the 4th generation Intel Xeon Scalable processor, visit the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI platform&lt;/a&gt; where you can learn how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;파이토치-리소스--pytorch-resources&quot;&gt;파이토치 리소스 / PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.kr/get-started/pytorch-2.0/&quot;&gt;파이토치 시작하기 (한국어)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;파이토치 개발자 포럼&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;파이토치 2.0 문서&lt;/a&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;http://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;제품-및-성능-정보--product-and-performance-information&quot;&gt;제품 및 성능 정보 / Product and Performance Information&lt;/h3&gt;

&lt;p&gt;1 Amazon EC2* m7i.16xlarge: 1-node, Intel Xeon Platinum 8488C processor with 256 GB memory (1 x 256 GB DDR5 4800 MT/s), microcode 0x2b000461, hyperthreading on, turbo on, Ubuntu* 22.04.3 LTS, kernel 6.2.0-1011-aws, GCC* 11.3.0, Amazon Elastic Block Store 200 GB, BIOS Amazon EC2 1.0 10/16/2017; Software: &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1&quot;&gt;PyTorch 2.1.0_rc4&lt;/a&gt;, &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/tree/v3.1.1&quot;&gt;Intel® oneAPI Deep Neural Network Library (oneDNN) version 3.1.1&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/benchmark/commit/ffbbebb9&quot;&gt;TorchBench&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision/commit/8636bf3&quot;&gt;TorchVision&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/text/commit/142d029&quot;&gt;TorchText&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/audio/commit/475b6ae&quot;&gt;TorchAudio&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/data/commit/eb9bf61&quot;&gt;TorchData&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1/benchmarks/dynamo&quot;&gt;TorchDynamo Benchmarks&lt;/a&gt;, tested by Intel on 9/12/2023.&lt;/p&gt;

&lt;p&gt;2 Amazon EC2 c6i.16xlarge: 1-node, Intel Xeon Platinum 8375C processor with 128 GB memory (1 x 128 GB DDR4 3200 MT/s), microcode 0xd0003a5, hyperthreading on, turbo on, Ubuntu 22.04.2 LTS, kernel 6.2.0-1011-aws, gcc 11.3.0, Amazon Elastic Block Store 200 GB, BIOS Amazon EC2 1.010/16/2017; Software: &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1&quot;&gt;PyTorch 2.1.0_rc4&lt;/a&gt;, &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/tree/v3.1.1&quot;&gt;oneDNN version 3.1.1&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/benchmark/commit/ffbbebb9&quot;&gt;TorchBench&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision/commit/8636bf3&quot;&gt;TorchVision&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/text/commit/142d029&quot;&gt;TorchText&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/audio/commit/475b6ae&quot;&gt;TorchAudio&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/data/commit/eb9bf61&quot;&gt;TorchData&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/release/2.1/benchmarks/dynamo&quot;&gt;TorchDynamo Benchmarks&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/benchmark/tree/chuanqiw/inductor_quant/userbenchmark/cpu&quot;&gt;TorchBench cpu userbenchmark&lt;/a&gt;, tested by Intel on 9/12/2023.&lt;/p&gt;</content><author><name>인텔(Intel)</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">PyTorch(파이토치) 2.1을 출시하여 매우 기쁩니다. 이번 글에서는 PyTorch 2.1에 인텔(Intel)이 크게 기여한 다섯가지 기능들에 대해서 설명하겠습니다: We are excited to see the release of PyTorch 2.1. In this blog, we discuss the five features for which Intel made significant contributions to PyTorch 2.1:</summary></entry><entry><title type="html">파이토치 재단(PyTorch Foundation) 1주년</title><link href="https://pytorch.kr/blog/2023/one-year-pytorch/" rel="alternate" type="text/html" title="파이토치 재단(PyTorch Foundation) 1주년" /><published>2023-09-12T00:00:00+09:00</published><updated>2023-09-12T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2023/one-year-pytorch</id><content type="html" xml:base="https://pytorch.kr/blog/2023/one-year-pytorch/">&lt;p&gt;파이토치 재단(PyTorch Foundation)의 설립을 발표한지 1년이 되었습니다! 🎉&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;It’s been one year since we announced the formation of the PyTorch Foundation! 🎉&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;창립 첫 해에 파이토치 재단은 PyTorch 2.0을 출시하고, 기여자를 늘리고, 새로운 회원사들이 합류하는 등의 상당한 성과를 거두었습니다. 재단이 발전할 수 있도록 지원해주신 창립 회원사들께 감사드립니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In its inaugural year, the PyTorch Foundation made a significant impact by launching PyTorch 2.0, growing contributors and adding new member companies. We’re grateful to our founding members for their support to move the foundation forward.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;지난 한 해 동안의 몇몇 이정표(milestone)는 다음과 같습니다:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;A few milestones in the past year include:&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;💻 GitHub 저장소 60만 개 돌파 / &lt;em&gt;Over 600,000 repositories on GitHub&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;✅ AI 구현의 60%가 PyTorch 선택 / &lt;em&gt;60% of AI implementations choosing PyTorch&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;📈 신규 저장소들이 전년 대비 20% 이상 성장 / &lt;em&gt;More than 20% year over year growth in new repositories&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;🤝 작년부터 12,000개 이상의 커밋 달성 / &lt;em&gt;Over 12,000 commits since last year&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;그리고 지난 한 해 동안 재단이 어떤 일을 해왔는지 살펴보시죠:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;And a look at what the foundation has been up to this past year:&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-timeline-ko.svg&quot; alt=&quot;파이토치 프로젝트가 걸어온 길 / PyTorch project timeline&quot; style=&quot;width:100%; max-width: 662px; display: block; margin-left: auto; margin-right: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리는 기여자들을 지원하고, AI 분야를 민주화하며, 새로운 혁신을 창출함으로써 앞으로 커뮤니티가 함께 성장할 수 있기를 기대하고 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We look forward to growing our community for the years to come through supporting our contributors, democratizing the AI field, and creating new innovations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;10월 16일, 17일에 샌프란시스코에서 열리는 올해 &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;파이토치 컨퍼런스(PyTorch Conference)&lt;/a&gt;에 여러분을 초대합니다. 컨퍼런스 등록이 빠르게 마감되고 있으니 이 흥미로운 행사에 참여할 수 있는 기회를 놓치지 마세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We invite you to join us at this year’s &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference&lt;/a&gt; on October 16-17 in San Francisco. Conference registration is filling up quickly, so take advantage of your chance to be part of this exciting event.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;파이토치 컨퍼런스(PyTorch Conference)에 참여하여 최신 발표에 대한 최신 정보를 얻고, 창립 회원사들 및 새롭게 파이토치 커뮤니티에 합류한 회원사들과 교류할 수 있는 기회를 잡으세요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Join us to stay informed about the latest announcements and have the opportunity to connect with both the founding members and new additions to the PyTorch community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;감사의 인사를 담아,&lt;br /&gt;
파이토치 재단 팀(The PyTorch Foundation Team)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;With thanks and gratitude,&lt;br /&gt;
The PyTorch Foundation Team&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>PyTorch Korea User Group</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">파이토치 재단(PyTorch Foundation)의 설립을 발표한지 1년이 되었습니다! 🎉 It’s been one year since we announced the formation of the PyTorch Foundation! 🎉</summary></entry><entry><title type="html">PyTorch에서 x86 CPU용 INT8 양자화</title><link href="https://pytorch.kr/blog/2023/int8-quantization/" rel="alternate" type="text/html" title="PyTorch에서 x86 CPU용 INT8 양자화" /><published>2023-08-07T00:00:00+09:00</published><updated>2023-08-07T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2023/int8-quantization</id><content type="html" xml:base="https://pytorch.kr/blog/2023/int8-quantization/">&lt;h2 id=&quot;개요--overview&quot;&gt;개요 / Overview&lt;/h2&gt;

&lt;p&gt;INT8 양자화(quantization)는 x86 CPU 플랫폼에서 딥러닝 추론 속도를 높이는 강력한 기법입니다. 모델의 가중치와 활성화의 정밀도를 32비트 부동소수점(FP32; 32-bit floating-point)에서 8비트 정수(INT8; 8-bit integer)로 줄임으로써 INT8 양자화는 정확도를 유지하면서도 추론 속도와 메모리 요구량을 크게 향상시킬 수 있었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;INT8 quantization is a powerful technique for speeding up deep learning inference on x86 CPU platforms. By reducing the precision of the model’s weights and activations from 32-bit floating-point (FP32) to 8-bit integer (INT8), INT8 quantization can significantly improve the inference speed and reduce memory requirements without sacrificing accuracy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이번 글에서는 파이토치(PyTorch)에서 x86 CPU용 INT8 양자화의 발전 상황에 대해서 논의해보겠습니다. 주로 새로운 x86 양자화 백엔드(quantization backend)에 초점을 맞추고, 파이토치 2.0 익스포트(PT2E; PyTorch 2.0 Export)와 TorchInductor(토치인덕터)를 사용한 새로운 양자화 경로에 대해서도 간략히 살펴보겠습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In this blog, we will discuss the recent progress on INT8 quantization for x86 CPU in PyTorch, focusing on the new x86 quantization backend. We will also briefly look at the new quantization path with PyTorch 2.0 Export (PT2E) and TorchInductor.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;x86-양자화-백엔드--x86-quantization-backend&quot;&gt;X86 양자화 백엔드 / X86 Quantization Backend&lt;/h2&gt;

&lt;p&gt;현재 파이토치(PyTorch)에서 권장하는 양자화 방법은 &lt;a href=&quot;http://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html?highlight=fx&quot;&gt;FX&lt;/a&gt;입니다. PyTorch 2.0 이전에는 x86 CPU의 기본 양자화 백엔드(일명 QEngine)는 FBGEMM(FB+GEMM; Facebook GEneral Matrix Multiplication)으로, 성능 향상을 위해 FBGEMM 라이브러리를 활용했습니다. PyTorch 2.0 출시 시에는 FBGEMM을 대체하기 위해 X86이라는 새로운 양자화 백엔드가 도입되었습니다. x86 양자화 백엔드는 FBGEMM과 &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onednn.html&quot;&gt;인텔(Intel)® oneDNN (oneAPI Deep Neural Network Library)&lt;/a&gt; 커널 라이브러리의 강점을 모두 활용하여, 기존 FBGEMM 백엔드에 비해 향상된 INT8 추론 성능을 제공합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The current recommended way of quantization in PyTorch is &lt;a href=&quot;http://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html?highlight=fx&quot;&gt;FX&lt;/a&gt;. Before PyTorch 2.0, the default quantization backend (a.k.a. QEngine) on x86 CPUs was FBGEMM, which leveraged the FBGEMM performance library to achieve the performance speedup. In the PyTorch 2.0 release, a new quantization backend called X86 was introduced to replace FBGEMM. The x86 quantization backend offers improved INT8 inference performance when compared to the original FBGEMM backend by leveraging the strengths of both FBGEMM and the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onednn.html&quot;&gt;Intel® oneAPI Deep Neural Network Library (oneDNN)&lt;/a&gt; kernel libraries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;x86-백엔드의-성능-향상--performance-benefit-from-x86-backend&quot;&gt;X86 백엔드의 성능 향상 / Performance Benefit from X86 Backend&lt;/h2&gt;

&lt;p&gt;새로운 X86 백엔드의 성능 향상을 측정하기 위해, &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;4세대 인텔(Intel)® 제온(Xeon)® 스케일러블 프로세서&lt;/a&gt;를 사용하여 인기있는 69종의 딥러닝 모델(아래 &lt;strong&gt;그림 1 ~ 그림 3&lt;/strong&gt; 참조)에 대해 INT8 추론을 실행했습니다. 그 결과, FP32 추론 성능에 비해 2.97배의 기하평균(geomean) 성능 속도 향상을 보였으며, FBGEMM 백엔드에서는 1.43배의 속도 향상을 보였습니다. 아래 차트는 x86 백엔드와 FBGEMM 백엔드를 비교한 모델별 성능 속도 향상을 보여줍니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To measure the performance benefits of the new X86 backend, we ran INT8 inference on 69 popular deep learning models (shown in &lt;strong&gt;Figures 1-3&lt;/strong&gt; below) using &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;4th Gen Intel® Xeon® Scalable processors&lt;/a&gt;. The results showed a 2.97X geomean performance speedup compared to FP32 inference performance, while the speedup was 1.43X with the FBGEMM backend. The charts below show the per-model performance speedup comparing the x86 backend and the FBGEMM backend.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_1.jpg&quot; alt=&quot;그림 1: x86 백엔드에서 2배 미만 성능 향상 모델 / Figure 1: Models with less than 2x performance boost with x86 backend&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;그림 1&lt;/strong&gt;: x86 백엔드에서 2배 미만 성능 향상 모델&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; / &lt;strong&gt;Figure 1&lt;/strong&gt;: Models with less than 2x performance boost with x86 backend&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_2.jpg&quot; alt=&quot;그림 2: x86 백엔드에서 2-4배 성능 향상 모델 / Figure 2: Models with 2x-4x performance boost with x86 backend&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;그림 2&lt;/strong&gt;: x86 백엔드에서 2-4배 성능 향상 모델&lt;sup id=&quot;fnref:1:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; / &lt;strong&gt;Figure 2&lt;/strong&gt;: Models with 2x-4x performance boost with x86 backend&lt;sup id=&quot;fnref:1:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int8/pytorch_quant_x86_3.jpg&quot; alt=&quot;그림 3: x86 백엔드에서 4배 이상의 성능 향상 모델 / Figure 3: Models with larger than 4x performance boost with x86 backend&quot; style=&quot;width:100%; margin-top: 4em;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;그림 3&lt;/strong&gt;: x86 백엔드에서 4배 이상의 성능 향상 모델&lt;sup id=&quot;fnref:1:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; / &lt;strong&gt;Figure 3&lt;/strong&gt;: Models with larger than 4x performance boost with x86 backend&lt;sup id=&quot;fnref:1:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;x86-백엔드-사용법--usage-of-x86-backend&quot;&gt;x86 백엔드 사용법 / Usage of x86 Backend&lt;/h2&gt;

&lt;p&gt;2.0에서는 기본적으로 x86 플랫폼 사용자는 x86 양자화 백엔드를 사용하며 기본 백엔드를 사용할 때는 프로그램 변경 없이도 동작합니다. 또는 사용자가 x86를 양자화 백엔드로 명시적(explitcit)으로 지정할 수 있습니다. &lt;br /&gt;
아래는 x86 양자화 백엔드를 사용한 PyTorch 정적 학습-후(post-training) 양자화 예제 코드입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;By default in 2.0, users on x86 platforms will use the x86 quantization backend and their PyTorch programs will remain unchanged when using the default backend. Alternatively, users can specify x86 as the quantization backend explicitly. &lt;br /&gt;
Below is an example code snippet of PyTorch static post-training quantization with x86 quantization backend.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.ao.quantization&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_default_qconfig_mapping&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.quantization.quantize_fx&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prepare_fx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_fx&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;qconfig_mapping&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_default_qconfig_mapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 또는, 명시적으로 qengine을 명시합니다 / Or explicity specify the qengine
# qengine = 'x86'
# torch.backends.quantized.engine = qengine
# qconfig_mapping = get_default_qconfig_mapping(qengine)
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model_fp32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# qconfig_mapping과 백엔드 설정에 따라 관찰자(observer)를 삽입합니다
# Insert observers according to qconfig and backend config
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepared_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prepare_fx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_fp32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qconfig_mapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example_inputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 캘리브레이션 코드는 표시하지 않음 / Calibration code not shown
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 양자화된 모델로 변환 / Convert to quantized model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantized_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_fx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepared_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;x86-백엔드의-기술적-세부-사항--technical-details-of-x86-backend&quot;&gt;x86 백엔드의 기술적 세부 사항 / Technical Details of x86 Backend&lt;/h2&gt;

&lt;p&gt;합성곱(convolution) 또는 행렬곱(matrix multiplication) 연산을 실행하기 위해서 oneDNN 또는 FBGEMM 성능 라이브러리 중 어떤 것을 호출할지 결정하기 위한 디스패치 규칙(dispatching rule)을 벤치마킹했던 모델들의 성능에 따라 휴리스틱하게 고안하였습니다. 이 규칙은 연산의 종류, 형태, CPU 아키텍처 정보 등을 고려하여 결정합니다. 자세한 로직은 &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/93ff71ec37e3c946603600a46edef70b42f81213/aten/src/ATen/native/quantized/cpu/OnednnUtils.h#L396&quot;&gt;여기&lt;/a&gt;에서 확인할 수 있습니다. 더 많은 설계 및 기술적 논의는 &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;RFC 문서(Request for Comments)&lt;/a&gt;를 참조하십시오.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We devised heuristic dispatching rules according to the performance numbers from the models we benchmarked to decide whether to invoke oneDNN or FBGEMM performance library to execute the convolution or matrix multiplication operations. The rules are a combination of operation kinds, shapes, CPU architecture information, etc. Detailed logic is available &lt;a href=&quot;http://github.com/pytorch/pytorch/blob/93ff71ec37e3c946603600a46edef70b42f81213/aten/src/ATen/native/quantized/cpu/OnednnUtils.h#L396&quot;&gt;here&lt;/a&gt;. For more design and technical discussion, please refer to the &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;Request for Comments&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;새로운-양자화-경로-pytorch-20-익스포트의-다음-단계--next-steps-with-a-new-quantization-path-pytorch-20-export&quot;&gt;새로운 양자화 경로, PyTorch 2.0 익스포트의 다음 단계 / Next Steps With a New Quantization Path PyTorch 2.0 Export&lt;/h2&gt;

&lt;p&gt;아직 확정되지는 않았지만, 새로운 양자화 경로인 파이토치 2.0 익스포트(PT2E; PyTorch 2.0 Export)가 현재 초기 설계 및 PoC 단계에 있으며, 향후 FX 양자화 경로를 대체할 것입니다. 이는 TorchDynamo Export라는 PyTorch 2.0에서 도입한 FX 그래프 캡쳐 기능을 기반으로 구축되었으며, 이 그래프는 양자화되어 다양한 백엔드들로 나눠(lowered)집니다. PyTorch의 새로운 DL 컴파일러인 TorchInductor는 x86 CPU에서 의미있는 FP32 추론 속도 향상을 보여주었으며, PT2E의 양자화 백엔드 중 하나로 만들기 위해 작업 중입니다. 이러한 새로운 경로가 다양한 계층(level)에서의 융합 가능성을 증대시켜 INT8 추론 성능을 더욱 향상시킬 것을 기대합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Although still far from finalized, a new quantization path, PyTorch 2.0 Export (PT2E), is in early design and PoC stage. The new approach is slated to replace the FX quantization path in the future. It is built upon the capabilities of TorchDynamo Export, a feature introduced in the PyTorch 2.0 release for FX graph capturing. This graph is then quantized and lowered to different backends. TorchInductor, the new DL compiler of PyTorch, has shown promising results in terms of FP32 inference speedup on x86 CPU. We are working actively to enable it as one of the quantization backends of PT2E. We believe the new path will lead to further improvements in INT8 inference performance due to more flexibility of fusion at different levels.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;결론--conclusion&quot;&gt;결론 / Conclusion&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 출시 시 도입된 x86 백엔드는 x86 CPU 플랫폼에서 놀라운 INT8 추론 속도의 향상을 보였습니다. 기존 FBGEMM 백엔드와 비교하여 1.43배의 속도 향상을 보이면서도 하위 호환성 또한 유지합니다. 이러한 성능 향상으로 최종 사용자는 프로그램을 약간 또는 전혀 수정하지 않고도 성능 향상을 누릴 수 있게 되었습니다. 또한, 현재 개발 중인 새로운 양자화 경로인 PT2E는 미래에 더 많은 가능성을 제공할 수 있을 것으로 기대합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The x86 backend introduced in PyTorch 2.0 release has demonstrated a remarkable improvement in INT8 inference speed on x86 CPU platforms. It offers a 1.43X speedup compared to the original FBGEMM backend while maintaining backward compatibility. This enhancement can benefit end users with minimal or no modifications to their programs. Furthermore, a new quantization path, PT2E, is currently in development and is expected to provide even more possibilities in the future.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;감사의-글--acknowledgement&quot;&gt;감사의 글 / Acknowledgement&lt;/h2&gt;

&lt;p&gt;Nikita Shulga, Vasiliy Kuznetsov, Supriya Rao 및 Jongsoo Park에게 특별히 감사드립니다. 이들과 함께 PyTorch CPU 생태계를 개선하기 위한 발걸음을 내딛을 수 있었습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Special thanks to Nikita Shulga, Vasiliy Kuznetsov, Supriya Rao, and Jongsoo Park. Together, we made one more step forward on the path of improving the PyTorch CPU ecosystem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;실험-구성--configuration&quot;&gt;실험 구성 / Configuration&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;AWS EC2 r7iz.metal-16xl instance (Intel(R) Xeon(R) Gold 6455B, 32-core/64-thread, Turbo Boost On, Hyper-Threading On, Memory: 8x64GB, Storage: 192GB); OS: Ubuntu 22.04.1 LTS; Kernel: 5.15.0-1028-aws; Batch Size: 1; Core per Instance: 4; PyTorch 2.0 RC3; TorchVision 0.15.0+cpu, 인텔이 2023년 3월 7일에 실험. 공개된 모든 보안 업데이트를 반영하지 않았을 수 있음(May not reflect all publicly available security updates). &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:1:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>인텔(Intel)</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html">개요 / Overview</summary></entry><entry><title type="html">파이토치 도카톤(Docathon) 2023 개최 안내</title><link href="https://pytorch.kr/blog/2023/announcing-docathon/" rel="alternate" type="text/html" title="파이토치 도카톤(Docathon) 2023 개최 안내" /><published>2023-05-03T00:00:00+09:00</published><updated>2023-05-03T00:00:00+09:00</updated><id>https://pytorch.kr/blog/2023/announcing-docathon</id><content type="html" xml:base="https://pytorch.kr/blog/2023/announcing-docathon/">&lt;p&gt;&lt;img src=&quot;/assets/blog/2023-05-03-announcing-docathon/docathon-cover.jpg&quot; alt=&quot;PyTorch Docathon&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 파이토치 도카톤(Docathon)을 개최하게 되어 매우 기쁩니다! 도카톤은 커뮤니티의 도움을 받아 문서를 개선하는 데 초점을 맞춘 해커톤 형식의 이벤트입니다. 문서는 모든 기술에서 매우 중요한 요소이며, 문서를 개선함으로써 사용자가 PyTorch를 더 쉽게 시작하고, 기능을 효과적으로 사용하는 방법을 이해하도록 돕고, 궁극적으로 머신러닝 분야의 연구부터 생산에 이르는 과정을 가속화할 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;We are excited to announce the first ever PyTorch Docathon! The Docathon is a hackathon-style event focused on improving the documentation by enlisting the help of the community. Documentation is a crucial aspect of any technology and by improving the documentation, we can make it easier for users to get started with PyTorch, help them understand how to use its features effectively, and ultimately accelerate research to production in the field of machine learning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;참여해야-하는-이유--why-participate&quot;&gt;참여해야 하는 이유 / WHY PARTICIPATE&lt;/h2&gt;

&lt;h3 id=&quot;낮은-진입-장벽--low-barrier-to-entry&quot;&gt;낮은 진입 장벽 / Low Barrier to Entry&lt;/h3&gt;

&lt;p&gt;많은 오픈소스 프로젝트는 해커톤 이벤트에 참여하기 위해 코드베이스에 대한 광범위한 지식과 프로젝트에 대한 사전 기여가 필요합니다. 반면에 문서톤(Docathon)은 초보자를 위해 설계되었습니다. Python에 대한 친숙함, PyTorch 및 ML에 대한 기본 지식이 있어야 합니다. 하지만 걱정하지 마세요! 웹사이트 이슈와 같은 일부 주제는 이마저도 필요하지 않으니까요.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Many open-source projects require extensive knowledge of the codebase and prior contributions to the project to participate in any sort of hackathon events. The Docathon, on the other hand, is designed for newcomers. We do expect familiarity with Python, basic knowledge of PyTorch, and ML. But don’t fret, there are some tasks that are related to website issues that won’t require even that.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;가시적인-결과--tangible-results&quot;&gt;가시적인 결과 / Tangible Results&lt;/h3&gt;

&lt;p&gt;문서톤(Docathon)의 가장 좋은 점 중 하나는 노력의 결과를 실시간으로 확인할 수 있다는 것입니다. 문서를 개선하는 것은 프로젝트의 사용성과 접근성에 큰 영향을 미칠 수 있으며, 이러한 개선 사항을 직접 확인할 수 있습니다. 또한 가시적인 성과는 계속 기여할 수 있는 큰 동기 부여가 되기도 합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;One of the best things about the Docathon is that you can see the results of your efforts in real time. Improving documentation can have a huge impact on a project’s usability and accessibility and you’ll be able to see those improvements firsthand. Plus having tangible results can be a great motivator to keep contributing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;협업-환경--collaborative-environment&quot;&gt;협업 환경 / Collaborative Environment&lt;/h3&gt;

&lt;p&gt;문서톤(Docathon)은 협업 이벤트이므로 다른 기여자 및 PyTorch 유지 관리자와 함께 문서 개선 작업을 할 수 있는 기회가 주어집니다. 다른 사람들로부터 배우고, 아이디어를 공유하고, 인맥을 쌓을 수 있는 좋은 기회입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The Docathon is a collaborative event which means you’ll have the opportunity to work with other contributors and PyTorch maintainers on improving the documentation. This can be a great way to learn from others, share ideas, and build connections.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;학습-기회--learning-opportunities&quot;&gt;학습 기회 / Learning Opportunities&lt;/h3&gt;

&lt;p&gt;마지막으로, 파이토치 전문가가 아니더라도 문서톤(Docathon)은 훌륭한 학습 경험이 될 수 있습니다. 파이토치 모듈들을 둘러보고 일부 튜토리얼들은 CI와 여러분의 컴퓨터에서 직접 테스트해볼 수 있습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Finally, even if you are not an expert in PyTorch, the Docathon can be a great learning experience. You’ll have the opportunity to explore the PyTorch modules and test some of the tutorials on your machine as well as in the CI.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;이벤트-상세--event-details&quot;&gt;이벤트 상세 / EVENT DETAILS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;5월 31일&lt;/strong&gt;: 문서톤(Docathon) 시작&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5월 31일 - 6월 11일&lt;/strong&gt;: 제출 및 피드백&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6월 12일 - 6월 13일&lt;/strong&gt;: 최종 검토&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6월 15일&lt;/strong&gt;: 수상자 발표
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;strong&gt;May 31&lt;/strong&gt;: Kick-off&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;May 31 - June 11&lt;/strong&gt;:  Submissions and Feedback&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;June 12 - June 13&lt;/strong&gt;: Final Reviews&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;June 15&lt;/strong&gt;: Winner Announcements&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;문서톤(Docathon)에 대한 자세한 내용은 5월 31일 킥오프 스트림에서 발표될 예정입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Details for the Docathon to be announced at the kick-off stream on May 31.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;올해 행사에 관심이 있으신가요? &lt;a href=&quot;https://community.linuxfoundation.org/e/mmbqqb/&quot;&gt;&lt;strong&gt;지금 등록&lt;/strong&gt;&lt;/a&gt;하세요!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Please register to join this year’s event: &lt;a href=&quot;https://community.linuxfoundation.org/e/mmbqqb/&quot;&gt;&lt;strong&gt;RSVP&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>PyTorch Korea User Group</name></author><category term="[&quot;pytorch.org&quot;, &quot;translation&quot;]" /><summary type="html"></summary></entry></feed>